import streamlit as st
import pandas as pd
#Load data from dataloader
from utils.data_loader import load_huspriser_dataset, load_diabetes_dataset, load_gletsjer_dataset, load_partikel_dataset
import os
from utils.config import DATA_PATHS
from utils.plots import plotting, plotting_glet, plotting_partikel, Plotting_class, plotting_reg_own, plotting_class_own 


#Importer pakker
# Data
import numpy as np
import scipy as scipy

# Plotting
import matplotlib.pyplot as plt

# Sklearn: et librabry med en masse funtioner vi bruger i Machine Learning
import sklearn as sklearn

# LightGBM - pakke til at k칮re decision tree
import lightgbm as lgb
from lightgbm import early_stopping
st.set_page_config(page_title="Avanceret Niveau", page_icon="游꿢")



def main():
    st.title("游꿢 Avanceret Niveau")

    # Load datasets using cached functions
    DS3 = load_gletsjer_dataset()
    DS4 = load_partikel_dataset()

    # Dataset selection
    st.sidebar.header("Datas칝t")
    dataset = st.sidebar.radio("V칝lg et datas칝t:", ["Gletsjer", "Partikel", "Upload eget datas칝t - Regression", "Upload eget datas칝t - Classification"])

    # Add description
    st.write('Alternativ til at k칮re .ipynb filen lokalt p친 din computer. Indeholder samme funktionaliteter som .ipynb filerne med uden at man skal skrive/se kode selv. ' \
    'Avanceret indeholder mulighed for at 칝ndre p친 de n칝vnte hyperparamtre i sp칮rgsm친lene plus v칝lge inputvariable.')
    st.write("V칝lg et datas칝t for at begynde.")    

    # Add a download link for guidance PDF in the sidebar
    pdf_path = 'data/vejledning.pdf'  # Put your PDF file here
    
    st.sidebar.write("") # Add vertical space above button

    #Add Download Buttons for PDFS

    # Download button for PDF HUSPRISER
    if os.path.exists(DATA_PATHS['VejledningHUSPRISER']):
        try:
            with open(DATA_PATHS['VejledningHUSPRISER'], "rb") as pdf_file:
                pdf_bytes = pdf_file.read()
            
            st.sidebar.download_button(
                label="游닌 Hent vejledning til Huspriser",
                data=pdf_bytes,
                file_name="vejledningHUSPRISER.pdf",
                mime="application/pdf"
            )
        except Exception as e:
            st.sidebar.error(f"Fejl ved indl칝sning af PDF: {e}")
    else:
        st.sidebar.warning("丘멆잺 Vejledning PDF ikke fundet.")

    # Download button for PDF DIABETES
    if os.path.exists(DATA_PATHS['VejledningDIABETES']):
        try:
            with open(DATA_PATHS['VejledningDIABETES'], "rb") as pdf_file:
                pdf_bytes = pdf_file.read()
            
            st.sidebar.download_button(
                label="游닌 Hent vejledning til Diabetes",
                data=pdf_bytes,
                file_name="vejledningDIABETES.pdf",
                mime="application/pdf"
            )
        except Exception as e:
            st.sidebar.error(f"Fejl ved indl칝sning af PDF: {e}")
    else:
        st.sidebar.warning("丘멆잺 Vejledning PDF ikke fundet.")
    
    # Download button for PDF GLETSJER
    if os.path.exists(DATA_PATHS['VejledningGLETSJER']):
        try:
            with open(DATA_PATHS['VejledningGLETSJER'], "rb") as pdf_file:
                pdf_bytes = pdf_file.read()
            
            st.sidebar.download_button(
                label="游닌 Hent vejledning til Gletsjer",
                data=pdf_bytes,
                file_name="vejledningGLETSJER.pdf",
                mime="application/pdf"
            )
        except Exception as e:
            st.sidebar.error(f"Fejl ved indl칝sning af PDF: {e}")
    else:
        st.sidebar.warning("丘멆잺 Vejledning PDF ikke fundet.")

    # Download button for PDF PARTIKEL
    if os.path.exists(DATA_PATHS['VejledningPARTIKEL']):
        try:
            with open(DATA_PATHS['VejledningPARTIKEL'], "rb") as pdf_file:
                pdf_bytes = pdf_file.read()
            
            st.sidebar.download_button(
                label="游닌 Hent vejledning til Partikel",
                data=pdf_bytes,
                file_name="vejledningPARTIKEL.pdf",
                mime="application/pdf"
            )
        except Exception as e:
            st.sidebar.error(f"Fejl ved indl칝sning af PDF: {e}")
    else:
        st.sidebar.warning("丘멆잺 Vejledning PDF ikke fundet.")
    
    # Content based on dataset - Standard level
    if dataset == "Gletsjer":
        #HER BEGYNDER VORES .ipynb
        st.subheader("Avanceret Niveau - Gletsjer")
        st.write("Nedenfor skal du hj칝lpe gletsjervidenskabsfakultetet med at udvikle deres ML model til at bestemme dybden af gletsjere. P친 denne hjemmeside beh칮ver vi ikke importere nogen pakker da det er tilrettelagt s친ledes at man skal kunne lege med ML-modellerne uden at skulle bekymre sig om koden bag dem.")

        #Inspicer dataen
        st.subheader("Inspicer dataen")
        st.write("F칮rst vil vi gerne unders칮ge hvilken data vi har med at g칮re.")
        st.dataframe(DS3, height=200, use_container_width=True)
        
        #Tilrettel칝g data
        #G칮r s친 jeg selv kan v칝lge hvilke varaible jeg vil have med

        st.write("V칝lg hvilke variable du vil bruge til at tr칝ne din model.")
        #Remove gletsjer_dybde from options
        options = [col for col in DS3.columns.tolist() if col != 'gletsjer_dybde']
        input_variabler = st.multiselect("V칝lg input variabler", options=options, default=options)
        variabler = input_variabler + ['gletsjer_dybde']
        input_data = DS3[input_variabler].to_numpy()
        truth_data = DS3['gletsjer_dybde'].to_numpy()

        st.subheader("Decision Tree")
        st.write("Et decision tree er bygget op af lag og grene. Ved hver gren stiller den et sp칮rgsm친l, og bev칝ger sig ned i det n칝ste lag baseret p친 om sp칮rgsm친let er sandt eller falsk. Og ved at l칝re af en masse data, kan den finde ud af hvilke sp칮rgsm친l der er bedst at stille.")

        st.subheader("Parameter")
        st.write("For et decision tree kan vi justere p친 hvor mange lag der skal v칝re i vores tr칝, alts친 hvor mange lag af sp칮rgsm친l der m친 stilles. Vi kan justere p친 den parameter herunder.")

        #Make a slider to choose depth
        DT_N_lag = st.slider("Antal lag i tr칝et", min_value=1, max_value=10, value=2, step=1)

        st.write("Her bygger og tr칝ner vi modellen og bruger Graphviz til at visualisere det.")

        # Her bliver modellen tr칝net p친 data
        estimator = sklearn.tree.DecisionTreeRegressor(max_depth=DT_N_lag, min_samples_leaf = 20,random_state=42)

        estimator.fit(input_data, truth_data)   # Dette er den "magiske" linje - her optimerer Machine Learning algoritmen sine interne v칝gte til at give bedste svar

        # laver visuel graf af tr칝et
        dot = sklearn.tree.export_graphviz(estimator, out_file=None, feature_names=input_variabler, filled=True, max_depth=50, precision=2)         
        dot = dot.replace("squared_error", "error").replace("mse", "error")
        st.graphviz_chart(dot)
        st.write("Max dybde af tr칝et:", estimator.get_depth())
        a = np.unique(estimator.predict(input_data)).size
        st.write("Forskellige dybder den kan forudsige:",a )

        st.subheader("Sp칮rgsm친l")
        st.markdown("""- Inspicer tr칝et. Forst친r du/I, hvad de forskellige tal betyder?
  Hvad er g칝ldende for gletsjerne i lag 2 og hvad er algortimens bud p친 deres dybde?
- Pr칮v at 칝ndre p친 hvor mange lag der er i tr칝et fra 2 til 3.
  Hvilken parameter bliver brugt oftest til at opdele data? Tror du/I at den s친 er den vigtigste parameter?
  Kan du/I ud fra tr칝et sige mere generelt hvilke parametre der betyder mest for dybden? Hvilke betyder mindst?""")
        
        st.subheader("Boosted Decision Tree")
        st.write("Nu hvor vi har set hvordan tr칝et virker, vil vi gerne pr칮ve at forudsige v칝rdien p친 gletsjere som vi ikke kender dybden p친. Som vi har set, kan det v칝re sv칝rt at minimere vores 'loss function'. En m친de at forbedre p친 er ved at k칮re boosted decision trees, hvilket vil sige at vi k칮rer flere tr칝er, hvor den hver gang l칝rer af fejlene fra det forrige tr칝, og p친 den m친de bliver 'boostet' for hvert tr칝 den laver. Herunder kan vi 칝ndre hvor mange gange den m친 'booste', alts친 hvor mange tr칝er den m친 lave og l칝re af.")
        
        boosting_rounds = st.slider("Antal boosting rounds", min_value=1, max_value=1000, value=1, step=1)
        st.write("Vi kan ogs친 v칝lge hvor stor en andel af data vi vil bruge. ")
        andel_af_data = st.slider("Andel af data til tr칝ning", min_value=0.001, max_value=1.0, value=1.0, step=0.001)
        
        #Vi omdefinerer vores input og truth data til kun at indeholde en del af dataene.
        input_data_justeret, truth_data_justeret = sklearn.utils.resample(
            input_data, truth_data, 
            n_samples=int(andel_af_data * len(input_data)), 
            random_state=42, 
            replace=False
            )
        st.write("""Vi splitter data i et tr칝ningss칝t og et tests칝t.
Tr칝ningss칝ttet bruges til at tr칝ne modellen, hvor modellen f친r de rigtige dybder at vide at vide.
Tests칝ttet bruges til at give den tr칝nede model data uden dybder, som den s친 skal forudsige, men hvor vi stadig kender svaret. Dette bruges til at evaluere modellens performance.""")
        data_tr칝ning, data_test, sand_dybde_tr칝ning, sand_dybde_test = \
    sklearn.model_selection.train_test_split(input_data_justeret, truth_data_justeret, test_size=0.25, random_state=42)
    
        # Her bygger vi modellen op med flere tr칝er, tr칝ner p친 data og forudsiger priser
        #Implement button to run below model
        st.subheader('Advanced - Hyperparametre')
        num_leaves = st.slider("Maksimalt antal kasser", min_value=10, max_value=100, value=31, step=1)
        boosting_type = st.selectbox("Hvilken algoritme bruger vi til at booste?", options=['gbdt', 'dart'], index=0)
        max_depth = st.slider("Hvor mange lad m친 der maksimalt v칝re i vores tr칝?", min_value=-1, max_value=100, value=-1, step=1)
        learning_rate_bdt = st.slider("Hvor store skridt tager modellen?", min_value=0.001, max_value=0.5, value=0.01, step=0.001)
        min_child_samples = st.slider("Minimum antal samples i hver kasse", min_value=1, max_value=100, value=20, step=1)
        
        if st.button("K칮r model"):
            gbm_test = lgb.LGBMRegressor( objective='regression', n_estimators=boosting_rounds,num_leaves=num_leaves, boosting_type=boosting_type, max_depth=max_depth, learning_rate=learning_rate_bdt, min_child_samples=min_child_samples, verbosity=-1)

            gbm_test.fit(data_tr칝ning, sand_dybde_tr칝ning, eval_set=[(data_test, sand_dybde_test)], 
                        eval_metric='mse', callbacks=[early_stopping(15)])

            forudsagt_dybde = gbm_test.predict(data_test, num_iteration=gbm_test.best_iteration_)
            plotting_glet(sand_dybde_test, forudsagt_dybde)

            res = sklearn.inspection.permutation_importance(gbm_test, data_test, sand_dybde_test, scoring="neg_mean_squared_error")

            st.write("Nu vil vi gerne inspicere hvor god vores model er til at forudsige p친 data hvor den ikke kender dybden i forvejen. Det venstre plot viser residualerne, alts친 (sand v칝rdi - forudsagt v칝rdi). Det h칮jre plot er sand v칝rdi vs forudsagt v칝rdi. Her er ogs친 konturer (de sorte linjer), der viser t칝theden af punkterne.")
            st.subheader("Sp칮rgsm친l")
            st.markdown("""
- Pr칮v at 칝ndre p친 hvor mange gange gange den m친 booste, ved at 칝ndre boosting_rounds fra 1 til 2 til 10, 100 eller 1000. Kan du se en forbedring?
- Hvilke gletsjere er der mest data p친?
- Hvad g칝tter modellen p친 hvis ikke den f칮r lov til at booste mange gange? Er der bestemte omr친der hvor modellen har sv칝rere ved at forudsige dybden?
- Leg rundt med andelen af data du bruger. Hvordan 칝ndres resultatet alt efter hvor meget data den har. Hvor meget data skal du bruge for at have en rimelig model og forudsigelse?""")
            st.subheader("Hvilke variable er vigtigst?")
            st.write("Vi kan tjekke om vores intuition for hvilke variable der er vigtigst med 'permutation importance'. Det er et m친l for hvis v칝rdierne i en kolonne bliver byttet rundt randomly, hvor meget p친virker det s친 resultatet. Hvis det er en vigtig variable, vil det p친virke resultatet meget. Her bliver det m친l p친 hvor meget st칮rre mean squared error bliver, n친r den variabel bliver 'scramblet'.")


            imp_mse = res.importances_mean                
            order = np.argsort(imp_mse)[::-1]
            labels = np.asarray(variabler[:-1])[order]
            vals = imp_mse[order]

            fig, ax = plt.subplots(figsize=(8, 6))
            y = np.arange(len(vals))
            ax.barh(y, vals)
            ax.set_yticks(y)
            ax.set_yticklabels(labels)
            ax.set_xlabel("Increase in MSE (permutation)")
            ax.set_ylabel("Feature")
            ax.set_title("Permutation Importance")
            ax.invert_yaxis()
            fig.tight_layout()
            st.pyplot(fig)

            st.markdown("""
- Er resultatet som du forventede? 
- Kan du give en mulig grund til hvorfor netop disse variable har st칮rst betydning?
                        """)

        #NN 
        st.subheader("Neurale Netv칝rk")
        st.write("Neurale Netv칝rk (NN) kommer fra at opbygningen af det, minder om den m친de vores neuroner i hjernen snakker sammen p친. P친 samme m친de som et decision tree er der forskellige lag og vi kan styre hvor mange lag der er, men nu er det ikke kun sandt eller falsk, i stedet fungerer noderne som knapper der kan fintunes.")
        st.write("Neurale netv칝rk er mere f칮lsomme overfor det data vi giver dem. Den fungerer bedst hvis v칝rdierne af data er mellem 0 og 1. Derfor bruger vi en funktion til at skalere vores data, kaldet StandardScaler.")
        scaler = sklearn.preprocessing.StandardScaler()
        data_tr칝ning = scaler.fit_transform(data_tr칝ning)
        data_test = scaler.transform(data_test)
        
    
        st.write("I et neuralt netv칝rk kan vi justere p친 hvor mange lag og hvor mange noder hvert lag skal have:")

        #Make six slider, one for each layer. that is six layers in total. sliders decide amount of nodes per layer
        layer_one = st.slider("Antal noder i lag 1", min_value=1, max_value=32, value=32, step=1)
        layer_two = st.slider("Antal noder i lag 2", min_value=1, max_value=32, value=16, step=1)
        layer_three = st.slider("Antal noder i lag 3", min_value=1, max_value=32, value=8, step=1)
        layer_four = st.slider("Antal noder i lag 4", min_value=1, max_value=32, value=4, step=1)
        layer_five = st.slider("Antal noder i lag 5", min_value=1, max_value=32, value=2, step=1)
        layer_six = st.slider("Antal noder i lag 6", min_value=1, max_value=32, value=2, step=1)

        st.subheader('Advanced - Hyperparametre')
        activation = st.selectbox("Hvilken activation function skal vi bruge?", options=['relu', 'tanh', 'logistic'], index=0)
        learning_rate_nn = st.selectbox("Hvilken slags learning rate skal vi bruge?", options=['constant', 'invscaling', 'adaptive'], index=0)
        max_iter = st.slider("Maksimalt antal iterationer (svarer til boosting_rounds for BDT)", min_value=1, max_value=2000, value=200, step=1)
        alpha = st.slider("Intern regulariseringsparameter for at forhindre overfitting", min_value=0.0001, max_value=0.1, value=0.0001, step=0.0001, format="%.4f")
        early_stopping_nn= st.checkbox("Brug early stopping?", value=True)
        st.write("""Nedenfor tr칝ner vi modellen. Vi kan ogs친 regne ud hvor mange parametre modellen bruger.
Herefter plotter vi for at se hvor godt modellen klarer sig.
                 Det kan godt tage op til ~et minut at k칮re denne model.""")
        if st.button("K칮r Neuralt Netv칝rk"):
            # Her definerer og tr칝ner vi modellen
            mlp = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(layer_one, layer_two, layer_three, layer_four, layer_five, layer_six), 
            max_iter=max_iter, activation=activation,learning_rate=learning_rate_nn, alpha = alpha, early_stopping=early_stopping_nn, random_state=42)
            mlp.fit(data_tr칝ning, sand_dybde_tr칝ning)
            # Her giver vi den tr칝nede model test data som den ikke har set f칮r, og beder om at forudsige dybden
            forudsagt_dybde = mlp.predict(data_test)  

            # Beregn antal parametre i modellen
            # Coef er v칝gtene er intercept er bias. Den henter antallet directe fra modellen.
            n_params = sum(coef.size + intercept.size for coef, intercept in zip(mlp.coefs_, mlp.intercepts_))
            st.write(f"Antal parametre i NN: {n_params}")
            plotting_glet(sand_dybde_test, forudsagt_dybde)
            st.subheader("Sp칮rgsm친l:")
            st.markdown("""
- Pr칮v at justere p친 antal neuroner i det neurale netv칝rk - Bliver modellen bedre d친rligere/k칮rer den hurtigere langsommere?
- F친r du det samme antal parametre n친r du regner efter?
- Hvilken algoritme klarer sig bedst? Boosted decision tree eller neutralt netv칝rk? Kan du f친 NN til at klare sig lige s친 godt som BDT? Eller omvendt?
- Leg rundt med andelen af data du bruger. Hvordan 칝ndres resultatet alt efter hvor meget data den har. Hvor meget data skal du bruge for at have en rimelig model og forudsigelse?
                        """)
            st.subheader("Avancerede Sp칮rgsm친l:")
            st.markdown("""
                        - Leg rundt med hyperparametre (HP) i begge modeller (BDT og NN). Tilf칮j og fjern, 칝ndr deres v칝rdier, k칮r modellen og se hvordan den performer.  
                        - Pr칮v at optimere dene s친 du f친r den bedste performance ved at pr칮ve forskellige kombinationer af HP af. Kan du komme i tanke op m친der man ville kunne optimere/strukturere denne process p친?
                        - Sl친 sklearn's GridSearchCV og RandomizedSearchCV op og find ud af hvad de g칮r. Hvad er fordele/ulemper ved begge?
                        - Fjern early_stopping. Hvad g칮r det. Er det en fordel? Kan du risikere at overtr칝ne hvis du ikke g칮r?
                        - Er alle kolonner lige vigtige? K칮r modellerne med kun de 5 bedste/v칝rste variable og se deres performance.
                        """)


    #Partikel
    elif dataset == "Partikel":
        #HER BEGYNDER VORES .ipynb
        st.subheader("Avanceret Niveau - Partikel")
        st.write("Som n칝vnt, er du blevet udn칝vnt personligt til at klassificere elektroner p친 CERN. P친 denne hjemmeside beh칮ver vi ikke importere nogen pakker da det er tilrettelagt s친ledes at man skal kunne lege med ML-modellerne uden at skulle bekymre sig om koden bag dem.")

        #Inspicer dataen
        st.subheader("Inspicer dataen")
        st.write("F칮rst vil vi gerne unders칮ge hvilken data vi har med at g칮re.")
        st.dataframe(DS4, height=200, use_container_width=True)
        
        #Tilrettel칝g data
        #G칮r s친 jeg selv kan v칝lge hvilke varaible jeg vil have med
        st.write("V칝lg hvilke variable du vil bruge til at tr칝ne din model.")
        #Remove pTruth_isElectron from options
        options = [col for col in DS4.columns.tolist() if col != 'p_Truth_isElectron']
        input_variable = st.multiselect("V칝lg input variabler", options=options, default=options)
        variable = input_variable + ['p_Truth_isElectron']
        input_data = DS4[input_variable].to_numpy()
        truth_data = DS4['p_Truth_isElectron'].to_numpy()

        st.subheader("Decision Tree")
        st.write("Et decision tree er bygget op af lag og grene. Ved hver gren stiller den et sp칮rgsm친l, og bev칝ger sig ned i det n칝ste lag baseret p친 om sp칮rgsm친let er sandt eller falsk. Og ved at l칝re af en masse data, kan den finde ud af hvilke sp칮rgsm친l der er bedst at stille.")

        st.subheader("Parameter")
        st.write("For et decision tree kan vi justere p친 hvor mange lag der skal v칝re i vores tr칝, alts친 hvor mange lag af sp칮rgsm친l der m친 stilles. Vi kan justere p친 den parameter herunder.")

        #Make a slider to choose depth
        DT_N_lag = st.slider("Antal lag i tr칝et", min_value=1, max_value=10, value=2, step=1)

        st.write("Her bygger og tr칝ner vi modellen og bruger Graphviz til at visualisere det.")

        # Her bliver modellen tr칝net p친 data
        estimator = sklearn.tree.DecisionTreeClassifier(max_depth=DT_N_lag, min_samples_leaf = 20,random_state=42)
        estimator.fit(input_data, truth_data)   # Dette er den "magiske" linje - her optimerer Machine Learning algoritmen sine interne v칝gte til at give bedste svar

        # laver visuel graf af tr칝et
        dot = sklearn.tree.export_graphviz(estimator, out_file=None, feature_names=input_variable, filled=True, max_depth=50, precision=2)         
        dot = dot.replace("squared_error", "error").replace("mse", "error")
        st.graphviz_chart(dot)
        st.write("Max dybde af tr칝et:", estimator.get_depth())


        st.subheader("Sp칮rgsm친l")
        st.markdown("""
- Inspicer tr칝et. Forst친r du/I, hvad de forskellige tal betyder?
  Hvad sker der fra lag til lag og hvor mange samples er der i hver kasse?
- Pr칮v at 칝ndre p친 hvor mange lag der er i tr칝et fra 2 til 3.
  Hvilke parametre bruges til at opdele data? 
- Hvordan 칝ndres v칝rdien af gini ift. om der kun er elektroner/ikke-elektroner eller begge typer?
                    """)
        
        st.subheader("Boosted Decision Tree")
        st.write("Nu hvor vi har set hvordan tr칝et virker, vil vi gerne pr칮ve at forudsige typen af partikler som vi ikke kender typen af p친 forh친nd. Som vi har set, kan det v칝re sv칝rt at minimere vores 'loss function'. En m친de at forbedre p친 er ved at k칮re boosted decision trees, hvilket vil sige at vi k칮rer flere tr칝er, hvor den hver gang l칝rer af fejlene fra det forrige tr칝, og p친 den m친de bliver 'boostet' for hvert tr칝 den laver. Herunder kan vi 칝ndre hvor mange gange den m친 'booste', alts친 hvor mange tr칝er den m친 lave og l칝rer af.")
        
        boosting_rounds = st.slider("Antal boosting rounds", min_value=1, max_value=1000, value=100, step=1)
        st.write("Vi kan ogs친 v칝lge hvor stor en andel af data vi vil bruge. ")
        andel_af_data = st.slider("Andel af data til tr칝ning", min_value=0.001, max_value=1.0, value=1.0, step=0.001)
        
        #Vi omdefinerer vores input og truth data til kun at indeholde en del af dataene.
        input_data_justeret, truth_data_justeret = sklearn.utils.resample(
            input_data, truth_data, 
            n_samples=int(andel_af_data * len(input_data)), 
            random_state=42, 
            replace=False
            )
        st.write("""Vi splitter data i et tr칝ningss칝t og et tests칝t.
Tr칝ningss칝ttet bruges til at tr칝ne modellen, hvor modellen f친r at vide om data er en elektron eller ej.
Tests칝ttet bruges til at give den tr칝nede model ny data (som den ikke kender svaret til), som den s친 skal forudsige, men hvor vi stadig kender svaret.""")
        data_train, data_test, label_train, label_test = \
    sklearn.model_selection.train_test_split(input_data_justeret, truth_data_justeret, test_size=0.25, random_state=42)
    
        # Her bygger vi modellen op med flere tr칝er, tr칝ner p친 data og forudsiger priser
        #Implement button to run below model
        st.subheader('Advanced - Hyperparametre')
        num_leaves = st.slider("Maksimalt antal kasser", min_value=10, max_value=100, value=31, step=1)
        boosting_type = st.selectbox("Hvilken algoritme bruger vi til at booste?", options=['gbdt', 'dart', 'rf'], index=0)
        max_depth = st.slider("Hvor mange lad m친 der maksimalt v칝re i vores tr칝?", min_value=-1, max_value=100, value=-1, step=1)
        learning_rate_bdt = st.slider("Hvor store skridt tager modellen?", min_value=0.001, max_value=0.5, value=0.01, step=0.001)
        min_child_samples = st.slider("Minimum antal samples i hver kasse", min_value=1, max_value=100, value=20, step=1)
        
        if st.button("K칮r model"):
            gbm_test = lgb.LGBMClassifier(n_estimators=boosting_rounds, num_leaves=num_leaves, max_depth=max_depth, learning_rate=learning_rate_bdt, min_child_samples=min_child_samples,
                              boosting_type=boosting_type, objective='binary', 
                              random_state=42)

            gbm_test.fit(data_train, label_train, eval_set=[(data_test, label_test)], 
            callbacks=[early_stopping(15)])

            # Her f친r vi sandsynlighederne for om hver person har diabetes eller ej
            Forudsigelse = gbm_test.predict_proba(data_test, num_iteration=gbm_test.best_iteration_)[:,1]
            
            plotting_partikel(label_test, Forudsigelse)


            st.subheader("Evaluer resultat med AUC og histogram")
            st.write("Nu vil vi gerne inspicere hvor god vores model er til at forusige p친 data hvor den ikke ved om data tilsvarer en elektron eller ej. Det venstre plot viser en ROC-kurve dvs. hvor stor en andel af sande g칝t har vi per andel af forkerte g칝t. Jo t칝ttere denne er p친 venstre 칮verste hj칮rne jo bedre. Dvs. n친r raten af forkerte g칝t er 0.1 er raten af korrekte g칝t allrede omkring 0.9.")
            st.write("Selve scoren Area Under Curve (AUC) angiver bare hvor t칝t p친 hj칮rnet grafen er. 1 angiver en perfekt score.")
            st.write("Det h칮jre plot viser fordelingen af korrekte og forkerte g칝t farvekodet efter hvad data rent faktisk svarede til. Dvs vi kigger p친 hvad modellen har g칝ttet p친 ud fra hvad vores data rent faktisk svarede til. Den r칮de linjer svarer til den gr칝nse modellen bruger til at afg칮re hvad den skal g칝tte p친 alt efter hvilken sandsynlighed den forudsiger.")
            st.subheader("Sp칮rgsm친l")
            st.markdown("""
- 칁ndr p친 antallet af boosting_rounds og se hvad der sker med modellene og resultatet. Kan du se forskel i performance for f.eks. 1, 10, 100, 1000 boosting_rounds?
- Hvad sker der med fordelingen af data i h칮jre plot n친r du 칝ndrer p친 boosting_rounds? Kan du stadig godt klassificere elektroner ved boosting_rounds=1 eller boosting_rounds=10? (Bem칝rk den stiplede linje er defineret ved 0.5 og modellen har ikke indflydelse p친 den.)
- Leg rundt med andelen af data du bruger. Hvordan 칝ndres resultatet alt efter hvor meget data den har. Hvor meget data skal du bruge for at have en rimelig model og forudsigelse?
                        """)
            st.subheader("Hvilke variabler er vigtigst?")
            st.write("Vi kan tjekke om vores hvilke variabler der er vigtigst for modellen til at lave en forudsigelse med 'permutation importance'. Det er et m친l for hvis v칝rdierne i en kolonne bliver byttet rundt randomly, hvor meget p친virker det s친 resultatet. Hvis det er en vigtig variabel, vil det p친virke resultatet meget. Her bliver det m친l p친 hvor meget st칮rre mean squared error bliver, n친r den variabel bliver 'scramblet'.")
            

            perm_importance = sklearn.inspection.permutation_importance(gbm_test, data_test, label_test,scoring='neg_log_loss', random_state=42)
            order = perm_importance.importances_mean.argsort()[::1]
            labels = np.asarray(variable[:-1])[order]
            vals = perm_importance.importances_mean[order]
            
            fig, ax = plt.subplots(figsize=(8, 6))
            y = np.arange(len(vals))
            ax.barh(y, vals)
            ax.set_yticks(y)
            ax.set_yticklabels(labels)
            ax.set_xlabel("Increase in log_loss (permutation)")
            ax.set_ylabel("Feature")
            ax.set_title("Permutation Importance")
            #ax.invert_yaxis()
            fig.tight_layout()
            st.pyplot(fig)

        #NN 
        st.subheader("Neurale Netv칝rk")
        st.write("Neurale Netv칝rk (NN) kommer fra at opbygningen af det, minder om den m친de vores neuroner i hjernen snakker sammen p친. P친 samme m친de som et decision tree er der forskellige lag og vi kan styre hvor mange lag der er, men nu er det ikke kun sandt eller falsk, i stedet fungerer noderne som knapper der kan fintunes. ")
        st.write("Neurale netv칝rk er mere f칮lsomme overfor det data vi giver dem. Den fungerer bedst hvis v칝rdierne af data er mellem 0 og 1. Derfor bruger vi en funktion til at skalere vores data, kaldet StandardScaler.")
        scaler = sklearn.preprocessing.StandardScaler()
        data_train_transformed = scaler.fit_transform(data_train)
        data_test_transformed = scaler.transform(data_test)    
        
        st.write("I et neuralt netv칝rk kan vi justere p친 hvor mange lag og hvor mange noder hvert lag skal have:")

        #Make six slider, one for each layer. that is six layers in total. sliders decide amount of nodes per layer
        layer_one = st.slider("Antal noder i lag 1", min_value=1, max_value=32, value=32, step=1)
        layer_two = st.slider("Antal noder i lag 2", min_value=1, max_value=32, value=16, step=1)
        layer_three = st.slider("Antal noder i lag 3", min_value=1, max_value=32, value=8, step=1)
        layer_four = st.slider("Antal noder i lag 4", min_value=1, max_value=32, value=4, step=1)
        layer_five = st.slider("Antal noder i lag 5", min_value=1, max_value=32, value=2, step=1)
        layer_six = st.slider("Antal noder i lag 6", min_value=1, max_value=32, value=2, step=1)

        st.subheader('Advanced - Hyperparametre')
        activation = st.selectbox("Hvilken activation function skal vi bruge?", options=['relu', 'tanh', 'logistic'], index=0)
        learning_rate_nn = st.selectbox("Hvilken slags learning rate skal vi bruge?", options=['constant', 'invscaling', 'adaptive'], index=0)
        max_iter = st.slider("Maksimalt antal iterationer (svarer til boosting_rounds for BDT)", min_value=1, max_value=2000, value=200, step=1)
        alpha = st.slider("Intern regulariseringsparameter for at forhindre overfitting", min_value=0.0001, max_value=0.1, value=0.0001, step=0.0001, format="%.4f")
        early_stopping_nn= st.checkbox("Brug early stopping?", value=True)
        
        st.write("""Nedenfor tr칝ner vi modellen. Vi kan ogs친 regne ud hvor mange parametre modellen bruger.
Herefter plotter vi for at se hvor godt modellen klarer sig.
                 Det kan godt tage op til ~et minut at k칮re denne model.""")
        if st.button("K칮r Neuralt Netv칝rk"):
            # Her definerer og tr칝ner vi modellen
            mlp = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(layer_one, layer_two, layer_three, layer_four, layer_five, layer_six), 
            max_iter=max_iter, activation=activation, learning_rate=learning_rate_nn, alpha=alpha, early_stopping=early_stopping_nn, random_state=42)
            mlp.fit(data_train_transformed, label_train) 

            # Her giver vi den tr칝nede model test data som den ikke har set f칮r, og beder om at forudsige prisen
            Forudsigelse = mlp.predict_proba(data_test_transformed)[:,1]

            # Beregn antal parametre i modellen
            # Coef er v칝gtene er intercept er bias. Den henter antallet direkte fra modellen.
            n_params = sum(coef.size + intercept.size for coef, intercept in zip(mlp.coefs_, mlp.intercepts_))
            st.write(f"Antal parametre i NN: {n_params}")
            plotting_partikel(label_test, Forudsigelse)
            st.subheader("Sp칮rgsm친l:")
            st.markdown("""
- Sammenlign modellen med boosted decision tree ovenover. Hvilken algoritme klarer sig bedst?
- 칁ndr antallet af neuroner per lag/antallet af lag og se hvordan performance 칝ndrer sig.
- F친r du det samme antal parametre n친r du regner efter?
- Leg rundt med andelen af data du bruger. Hvordan 칝ndres resultatet alt efter hvor meget data den har. Hvor meget data skal du bruge for at have en rimelig model og forudsigelse?""")
            st.subheader("Avancerede Sp칮rgsm친l:")
            st.markdown("""
                        - Leg rundt med hyperparametre (HP) i begge modeller (BDT og NN). Tilf칮j og fjern, 칝ndr deres v칝rdier, k칮r modellen og se hvordan den performer.  
                        - Pr칮v at optimere dene s친 du f친r den bedste performance ved at pr칮ve forskellige kombinationer af HP af. Kan du komme i tanke op m친der man ville kunne optimere/strukturere denne process p친?
                        - Sl친 sklearn's GridSearchCV og RandomizedSearchCV op og find ud af hvad de g칮r. Hvad er fordele/ulemper ved begge?
                        - Fjern early_stopping. Hvad g칮r det. Er det en fordel? Kan du risikere at overtr칝ne hvis du ikke g칮r?
                        - Er alle kolonner lige vigtige? K칮r modellerne med kun de 5 bedste/v칝rste variable og se deres performance.
                        """)

    if dataset == "Upload eget datas칝t - Regression":
        #HER BEGYNDER VORES .ipynb
        st.subheader("Upload eget datas칝t - Regression")
        st.write("Her kan du uploade eget datas칝t og k칮re tilh칮rende ML modeller p친 det.")

        #Inspicer dataen
        st.subheader("Inspicer dataen")
        #Let the user upload their own csv file
        uploaded_file = st.file_uploader("Upload din egen CSV fil her", type=["csv"])
        if uploaded_file is not None:
            DS_OWN = pd.read_csv(uploaded_file)
            st.dataframe(DS_OWN, height=200, use_container_width=True)

        #Tilrettel칝g data
        #G칮r s친 jeg selv kan v칝lge hvilke varaible jeg vil have med

        st.write("V칝lg hvilke variable du vil bruge til at tr칝ne din model og hvad vil du lave regression mod?")
        #Let user to choose variable to do regression on
        if uploaded_file is not None:
            options = DS_OWN.columns.tolist()
            target_variable = st.selectbox("V칝lg target variabel (den du vil lave regression mod)", options=options, index=len(options)-1)
            input_variable = st.multiselect("V칝lg input variabler", options=[col for col in options if col != target_variable], default=[col for col in options if col != target_variable])
            variable = input_variable + [target_variable]
            input_data = DS_OWN[input_variable].to_numpy()
            truth_data = DS_OWN[target_variable].to_numpy()
            #HER RYKKER VI ALT IND FOR AT UNDG칀 FEJL______________________________________________ 
            st.subheader("Boosted Decision Tree")
            st.write("Valg af hyperparametre for BDT.")

            andel_af_data = st.slider("Andel af data til tr칝ning", min_value=0.001, max_value=1.0, value=1.0, step=0.001)
            #Vi omdefinerer vores input og truth data til kun at indeholde en del af dataene.
            input_data_justeret, truth_data_justeret = sklearn.utils.resample(
                input_data, truth_data, 
                n_samples=int(andel_af_data * len(input_data)), 
                random_state=42, 
                replace=False
                )
            data_tr칝ning, data_test, sand_dybde_tr칝ning, sand_dybde_test = \
            sklearn.model_selection.train_test_split(input_data_justeret, truth_data_justeret, test_size=0.25, random_state=42)

            boosting_rounds = st.slider("Antal boosting rounds", min_value=1, max_value=1000, value=1, step=1)
            num_leaves = st.slider("Maksimalt antal kasser", min_value=10, max_value=100, value=31, step=1)
            boosting_type = st.selectbox("Hvilken algoritme bruger vi til at booste?", options=['gbdt', 'dart'], index=0)
            max_depth = st.slider("Hvor mange lad m친 der maksimalt v칝re i vores tr칝?", min_value=-1, max_value=100, value=-1, step=1)
            learning_rate_bdt = st.slider("Hvor store skridt tager modellen?", min_value=0.001, max_value=0.5, value=0.01, step=0.001)
            min_child_samples = st.slider("Minimum antal samples i hver kasse", min_value=1, max_value=100, value=20, step=1)

            if st.button("K칮r model"):
                gbm_test = lgb.LGBMRegressor( objective='regression', n_estimators=boosting_rounds,num_leaves=num_leaves, boosting_type=boosting_type, max_depth=max_depth, learning_rate=learning_rate_bdt, min_child_samples=min_child_samples, verbosity=-1)

                gbm_test.fit(data_tr칝ning, sand_dybde_tr칝ning, eval_set=[(data_test, sand_dybde_test)], 
                            eval_metric='mse', callbacks=[early_stopping(15)])

                forudsagt_dybde = gbm_test.predict(data_test, num_iteration=gbm_test.best_iteration_)
                plotting_reg_own(sand_dybde_test, forudsagt_dybde)

                res = sklearn.inspection.permutation_importance(gbm_test, data_test, sand_dybde_test, scoring="neg_mean_squared_error")

                st.subheader("Permutation Importance")

                imp_mse = res.importances_mean                
                order = np.argsort(imp_mse)[::-1]
                labels = np.asarray(variable[:-1])[order]
                vals = imp_mse[order]

                fig, ax = plt.subplots(figsize=(8, 6))
                y = np.arange(len(vals))
                ax.barh(y, vals)
                ax.set_yticks(y)
                ax.set_yticklabels(labels)
                ax.set_xlabel("Increase in MSE (permutation)")
                ax.set_ylabel("Feature")
                ax.set_title("Permutation Importance")
                ax.invert_yaxis()
                fig.tight_layout()
                st.pyplot(fig)

            #NN 
            st.subheader("Neurale Netv칝rk")

            scaler = sklearn.preprocessing.StandardScaler()
            data_tr칝ning = scaler.fit_transform(data_tr칝ning)
            data_test = scaler.transform(data_test)


            st.subheader("Valg af hyperparametre for NN.")

            #Make six slider, one for each layer. that is six layers in total. sliders decide amount of nodes per layer
            layer_one = st.slider("Antal noder i lag 1", min_value=1, max_value=128, value=32, step=1)
            layer_two = st.slider("Antal noder i lag 2", min_value=1, max_value=128, value=16, step=1)
            layer_three = st.slider("Antal noder i lag 3", min_value=1, max_value=128, value=8, step=1)
            layer_four = st.slider("Antal noder i lag 4", min_value=1, max_value=128, value=4, step=1)
            layer_five = st.slider("Antal noder i lag 5", min_value=1, max_value=128, value=2, step=1)
            layer_six = st.slider("Antal noder i lag 6", min_value=1, max_value=128, value=2, step=1)

            activation = st.selectbox("Hvilken activation function skal vi bruge?", options=['relu', 'tanh', 'logistic'], index=0)
            learning_rate_nn = st.selectbox("Hvilken slags learning rate skal vi bruge?", options=['constant', 'invscaling', 'adaptive'], index=0)
            max_iter = st.slider("Maksimalt antal iterationer (svarer til boosting_rounds for BDT)", min_value=1, max_value=2000, value=200, step=1)
            alpha = st.slider("Intern regulariseringsparameter for at forhindre overfitting", min_value=0.0001, max_value=0.1, value=0.0001, step=0.0001, format="%.4f")
            early_stopping_nn= st.checkbox("Brug early stopping?", value=True)

            if st.button("K칮r Neuralt Netv칝rk"):
                # Her definerer og tr칝ner vi modellen
                mlp = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(layer_one, layer_two, layer_three, layer_four, layer_five, layer_six), 
                max_iter=max_iter, activation=activation,learning_rate=learning_rate_nn, alpha = alpha, early_stopping=early_stopping_nn, random_state=42)
                mlp.fit(data_tr칝ning, sand_dybde_tr칝ning)
                # Her giver vi den tr칝nede model test data som den ikke har set f칮r, og beder om at forudsige dybden
                forudsagt_dybde = mlp.predict(data_test)  

                # Beregn antal parametre i modellen
                # Coef er v칝gtene er intercept er bias. Den henter antallet directe fra modellen.
                n_params = sum(coef.size + intercept.size for coef, intercept in zip(mlp.coefs_, mlp.intercepts_))
                st.write(f"Antal parametre i NN: {n_params}")
                plotting_reg_own(sand_dybde_test, forudsagt_dybde)

    elif dataset == "Upload eget datas칝t - Classification":
        #HER BEGYNDER VORES .ipynb
        st.subheader("Upload eget datas칝t - Classification")
        st.write("Her kan du uploade eget datas칝t og k칮re tilh칮rende ML modeller p친 det.")

        #Inspicer dataen
        st.subheader("Inspicer dataen")
        #Add possibility for user to upload their own csv file
        uploaded_file = st.file_uploader("Upload din egen CSV fil her", type=["csv"])
        if uploaded_file is not None:
            DS_OWN = pd.read_csv(uploaded_file)
            st.dataframe(DS_OWN, height=200, use_container_width=True)
        
        st.write("V칝lg hvilke variable du vil bruge til at tr칝ne din model og hvad vil du lave regression mod?")
        #Let user to choose variable to do regression on
        if uploaded_file is not None:
            options = DS_OWN.columns.tolist()
            target_variable = st.selectbox("V칝lg target variabel (den du vil lave regression mod)", options=options, index=len(options)-1)
            input_variable = st.multiselect("V칝lg input variabler", options=[col for col in options if col != target_variable], default=[col for col in options if col != target_variable])
            variable = input_variable + [target_variable]
            input_data = DS_OWN[input_variable].to_numpy()
            truth_data = DS_OWN[target_variable].to_numpy()
            #HER RYKKER VI ALT IND FOR AT UNDG칀 FEJL______________________________________________
            st.subheader("Boosted Decision Tree")
            st.write("V칝lg af hyperparametre for BDT.")
            
            andel_af_data = st.slider("Andel af data til tr칝ning", min_value=0.001, max_value=1.0, value=1.0, step=0.001)
            #Vi omdefinerer vores input og truth data til kun at indeholde en del af dataene.
            input_data_justeret, truth_data_justeret = sklearn.utils.resample(
                input_data, truth_data, 
                n_samples=int(andel_af_data * len(input_data)), 
                random_state=42, 
                replace=False
                )
            data_train, data_test, label_train, label_test = \
            sklearn.model_selection.train_test_split(input_data_justeret, truth_data_justeret, test_size=0.25, random_state=42)
    
            boosting_rounds = st.slider("Antal boosting rounds", min_value=1, max_value=1000, value=100, step=1)
            num_leaves = st.slider("Maksimalt antal kasser", min_value=10, max_value=100, value=31, step=1)
            boosting_type = st.selectbox("Hvilken algoritme bruger vi til at booste?", options=['gbdt', 'dart', 'rf'], index=0)
            max_depth = st.slider("Hvor mange lad m친 der maksimalt v칝re i vores tr칝?", min_value=-1, max_value=100, value=-1, step=1)
            learning_rate_bdt = st.slider("Hvor store skridt tager modellen?", min_value=0.001, max_value=0.5, value=0.01, step=0.001)
            min_child_samples = st.slider("Minimum antal samples i hver kasse", min_value=1, max_value=100, value=20, step=1)
            
            beslutningsgr칝nse = st.slider("Beslutningsgr칝nse", min_value=0.0, max_value=1.0, value=0.5, step=0.01)
            
            if st.button("K칮r model"):
                gbm_test = lgb.LGBMClassifier(n_estimators=boosting_rounds, num_leaves=num_leaves, max_depth=max_depth, learning_rate=learning_rate_bdt, min_child_samples=min_child_samples,
                                  boosting_type=boosting_type, objective='binary', 
                                  random_state=42)
    
                gbm_test.fit(data_train, label_train, eval_set=[(data_test, label_test)], 
                callbacks=[early_stopping(15)])
    
                # Her f친r vi sandsynlighederne for om hver person har diabetes eller ej
                Forudsigelse = gbm_test.predict_proba(data_test, num_iteration=gbm_test.best_iteration_)[:,1]
                forudsagte_klasse = gbm_test.predict_proba(data_test, num_iteration=gbm_test.best_iteration_)[:,1]
                forudsagte_klasse = (forudsagte_klasse > beslutningsgr칝nse).astype(int)
    
                plotting_class_own(label_test, Forudsigelse, forudsagte_klasse, beslutningsgr칝nse)
    
    
                st.subheader("Permutation Importance")            
    
                perm_importance = sklearn.inspection.permutation_importance(gbm_test, data_test, label_test,scoring='neg_log_loss', random_state=42)
                order = perm_importance.importances_mean.argsort()[::1]
                labels = np.asarray(variable[:-1])[order]
                vals = perm_importance.importances_mean[order]
                
                fig, ax = plt.subplots(figsize=(8, 6))
                y = np.arange(len(vals))
                ax.barh(y, vals)
                ax.set_yticks(y)
                ax.set_yticklabels(labels)
                ax.set_xlabel("Increase in log_loss (permutation)")
                ax.set_ylabel("Feature")
                ax.set_title("Permutation Importance")
                #ax.invert_yaxis()
                fig.tight_layout()
                st.pyplot(fig)
    
            #NN 
            st.subheader("Neurale Netv칝rk")
            scaler = sklearn.preprocessing.StandardScaler()
            data_train_transformed = scaler.fit_transform(data_train)
            data_test_transformed = scaler.transform(data_test)    
            
            st.subheader("Valg af hyperparametre for NN.")
            #Make six slider, one for each layer. that is six layers in total. sliders decide amount of nodes per layer
            layer_one = st.slider("Antal noder i lag 1", min_value=1, max_value=128, value=32, step=1)
            layer_two = st.slider("Antal noder i lag 2", min_value=1, max_value=128, value=16, step=1)
            layer_three = st.slider("Antal noder i lag 3", min_value=1, max_value=128, value=8, step=1)
            layer_four = st.slider("Antal noder i lag 4", min_value=1, max_value=128, value=4, step=1)
            layer_five = st.slider("Antal noder i lag 5", min_value=1, max_value=128, value=2, step=1)
            layer_six = st.slider("Antal noder i lag 6", min_value=1, max_value=128, value=2, step=1)
    
            activation = st.selectbox("Hvilken activation function skal vi bruge?", options=['relu', 'tanh', 'logistic'], index=0)
            learning_rate_nn = st.selectbox("Hvilken slags learning rate skal vi bruge?", options=['constant', 'invscaling', 'adaptive'], index=0)
            max_iter = st.slider("Maksimalt antal iterationer (svarer til boosting_rounds for BDT)", min_value=1, max_value=2000, value=200, step=1)
            alpha = st.slider("Intern regulariseringsparameter for at forhindre overfitting", min_value=0.0001, max_value=0.1, value=0.0001, step=0.0001, format="%.4f")
            early_stopping_nn= st.checkbox("Brug early stopping?", value=True)
            beslutningsgr칝nse_nn = st.slider("Beslutningsgr칝nse ", min_value=0.0, max_value=1.0, value=0.5, step=0.01)        
            if st.button("K칮r Neuralt Netv칝rk"):
                # Her definerer og tr칝ner vi modellen
                mlp = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(layer_one, layer_two, layer_three, layer_four, layer_five, layer_six), 
                max_iter=max_iter, activation=activation, learning_rate=learning_rate_nn, alpha=alpha, early_stopping=early_stopping_nn, random_state=42)
                mlp.fit(data_train_transformed, label_train) 
    
                # Her giver vi den tr칝nede model test data som den ikke har set f칮r, og beder om at forudsige prisen
                Forudsigelse = mlp.predict_proba(data_test_transformed)[:,1]
                forudsagte_klasse_nn = mlp.predict_proba(data_test_transformed)[:,1]
                forudsagte_klasse_nn = (forudsagte_klasse_nn > beslutningsgr칝nse_nn).astype(int)
    
                # Beregn antal parametre i modellen
                # Coef er v칝gtene er intercept er bias. Den henter antallet direkte fra modellen.
                n_params = sum(coef.size + intercept.size for coef, intercept in zip(mlp.coefs_, mlp.intercepts_))
                st.write(f"Antal parametre i NN: {n_params}")
                plotting_class_own(label_test, Forudsigelse, forudsagte_klasse_nn, beslutningsgr칝nse_nn)
            

if __name__ == "__main__":
    main()