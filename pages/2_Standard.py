import streamlit as st
import pandas as pd
#Load data from dataloader
from utils.data_loader import load_huspriser_dataset, load_diabetes_dataset, load_gletsjer_dataset, load_partikel_dataset
import os
from utils.config import DATA_PATHS
from utils.plots import plotting
from utils.plots import Plotting_class
from utils.plots import plotting_partikel

#Importer pakker
# Data
import numpy as np
import scipy as scipy

# Plotting
import matplotlib.pyplot as plt

# Sklearn: et librabry med en masse funtioner vi bruger i Machine Learning
import sklearn as sklearn

# LightGBM - pakke til at k칮re decision tree
import lightgbm as lgb
from lightgbm import early_stopping
st.set_page_config(page_title="Standard Niveau", page_icon="游꿢")



def main():
    st.title("游꿢 Standard Niveau")

    # Load datasets using cached functions
    DS1 = load_huspriser_dataset()
    DS2 = load_diabetes_dataset()
    DS3 = load_gletsjer_dataset()
    DS4 = load_partikel_dataset()

    # Dataset selection
    st.sidebar.header("Datas칝t")
    dataset = st.sidebar.radio("V칝lg et datas칝t:", ["Huspriser", "Diabetes", "Gletsjer", "Partikel"])

    # Add description
    st.write('Alternativ til at k칮re .ipynb filen lokalt p친 din computer. Indeholder samme funktionaliteter som .ipynb filerne med uden at man skal skrive/se kode selv.')
    st.write("V칝lg et datas칝t for at begynde.")    

    # Add a download link for guidance PDF in the sidebar
    pdf_path = 'data/vejledning.pdf'  # Put your PDF file here
    
    st.sidebar.write("") # Add vertical space above button

    #Add Download Buttons for PDFS

    # Download button for PDF HUSPRISER
    if os.path.exists(DATA_PATHS['VejledningHUSPRISER']):
        try:
            with open(DATA_PATHS['VejledningHUSPRISER'], "rb") as pdf_file:
                pdf_bytes = pdf_file.read()
            
            st.sidebar.download_button(
                label="游닌 Hent vejledning til Huspriser",
                data=pdf_bytes,
                file_name="vejledningHUSPRISER.pdf",
                mime="application/pdf"
            )
        except Exception as e:
            st.sidebar.error(f"Fejl ved indl칝sning af PDF: {e}")
    else:
        st.sidebar.warning("丘멆잺 Vejledning PDF ikke fundet.")

    # Download button for PDF DIABETES
    if os.path.exists(DATA_PATHS['VejledningDIABETES']):
        try:
            with open(DATA_PATHS['VejledningDIABETES'], "rb") as pdf_file:
                pdf_bytes = pdf_file.read()
            
            st.sidebar.download_button(
                label="游닌 Hent vejledning til Diabetes",
                data=pdf_bytes,
                file_name="vejledningDIABETES.pdf",
                mime="application/pdf"
            )
        except Exception as e:
            st.sidebar.error(f"Fejl ved indl칝sning af PDF: {e}")
    else:
        st.sidebar.warning("丘멆잺 Vejledning PDF ikke fundet.")
    
    # Download button for PDF GLETSJER
    if os.path.exists(DATA_PATHS['VejledningGLETSJER']):
        try:
            with open(DATA_PATHS['VejledningGLETSJER'], "rb") as pdf_file:
                pdf_bytes = pdf_file.read()
            
            st.sidebar.download_button(
                label="游닌 Hent vejledning til Gletsjer",
                data=pdf_bytes,
                file_name="vejledningGLETSJER.pdf",
                mime="application/pdf"
            )
        except Exception as e:
            st.sidebar.error(f"Fejl ved indl칝sning af PDF: {e}")
    else:
        st.sidebar.warning("丘멆잺 Vejledning PDF ikke fundet.")

    # Download button for PDF PARTIKEL
    if os.path.exists(DATA_PATHS['VejledningPARTIKEL']):
        try:
            with open(DATA_PATHS['VejledningPARTIKEL'], "rb") as pdf_file:
                pdf_bytes = pdf_file.read()
            
            st.sidebar.download_button(
                label="游닌 Hent vejledning til Partikel",
                data=pdf_bytes,
                file_name="vejledningPARTIKEL.pdf",
                mime="application/pdf"
            )
        except Exception as e:
            st.sidebar.error(f"Fejl ved indl칝sning af PDF: {e}")
    else:
        st.sidebar.warning("丘멆잺 Vejledning PDF ikke fundet.")
    
    # Content based on dataset - Standard level
    if dataset == "Huspriser":
        #HER BEGYNDER VORES .ipynb
        st.subheader("Standard Niveau - Huspriser")
        st.write("Som n칝vnt, har du f친et betroet opgaven at skrive en machine learning algoritme der kan forudsiger priser p친 huse. P친 denne hjemmeside beh칮ver vi ikke importere nogen pakker da det er tilrettelagt s친ledes at man skal kunne lege med ML-modellerne uden at skulle bekymre sig om koden bag dem.")

        st.subheader("Inspicer dataen")
        st.write("F칮rst vil vi gerne unders칮ge hvilken data vi har med at g칮re. V칝r opm칝rksom p친 at salgsprisen er i hele millioner.")
        st.dataframe(DS1, height=200, use_container_width=True)
        
        #Tilrettel칝g data
        variabler = DS1.columns
        input_variabler = [v for v in variabler if v != 'Salgspris']
        input_data = DS1[input_variabler].to_numpy()
        truth_data = DS1['Salgspris'].to_numpy()

        st.subheader("Decision Tree")
        st.write("Et decision tree er bygget op af lag og grene. Ved hver gren stiller den et sp칮rgsm친l, og bev칝ger sig ned i det n칝ste lag baseret p친 om sp칮rgsm친let er sandt eller falsk. Og ved at l칝re af en masse data, kan den finde ud af hvilke sp칮rgsm친l der er bedst at stille.")

        st.subheader("Parameter")
        st.write("For et decision tree kan vi justere p친 hvor mange lag der skal v칝re i vores tr칝, alts친 hvor mange lag af sp칮rgsm친l der m친 stilles. Vi kan justere p친 den parameter herunder.")

        #Make a slider to choose depth
        DT_N_lag = st.slider("Antal lag i tr칝et", min_value=1, max_value=10, value=2, step=1)

        st.write("Her bygger og tr칝ner vi modellen og bruger Graphviz til at visualisere det.")

        # Her bliver modellen tr칝net p친 data
        estimator = sklearn.tree.DecisionTreeRegressor(max_depth=DT_N_lag, min_samples_leaf = 20,random_state=42)

        estimator.fit(input_data, truth_data)   # Dette er den "magiske" linje - her optimerer Machine Learning algoritmen sine interne v칝gte til at give bedste svar

        # laver visuel graf af tr칝et
        dot = sklearn.tree.export_graphviz(estimator, out_file=None, feature_names=input_variabler, filled=True, max_depth=50, precision=2)         
        dot = dot.replace("squared_error", "error").replace("mse", "error")
        st.graphviz_chart(dot)
        st.write("Max dybde af tr칝et:", estimator.get_depth())
        a = np.unique(estimator.predict(input_data)).size
        st.write("Forskellige priser den kan forudsige:",a )

        st.subheader("Sp칮rgsm친l")
        st.markdown("""- Inspicer tr칝et. Forst친r du/I, hvad de forskellige tal betyder?
Hvilken type bolig passer flest eksempler ned i, i lag 2? Hvad er algoritmens bud p친 deres pris (dvs. gennemsnitsprisen)?
- Pr칮v at 칝ndre p친 hvor mange lag der er i tr칝et fra 2 til 3.
Hvilken parameter bliver brugt oftest til at opdele data? Tror du/I at den s친 er den vigtigste parameter?
Kan du/I ud fra tr칝et sige mere generelt hvilke parametre der betyder mest for prisen? Hvilke betyder mindst?""")
        
        st.subheader("Boosted Decision Tree")
        st.write("Nu hvor vi har set hvordan tr칝et virker, vil vi gerne pr칮ve at forudsige v칝rdien p친 huse som vi ikke kender salgsprisen p친. Som vi har set, kan det v칝re sv칝rt at minimere vores 'loss function'. En m친de at forbedre p친 er ved at k칮re boosted decision trees, hvilket vil sige at vi k칮rer flere tr칝er, hvor den hver gang l칝rer af fejlene fra det forrige tr칝, og p친 den m친de bliver 'boostet' for hvert tr칝 den laver. Herunder kan vi 칝ndre hvor mange gange den m친 'booste', alts친 hvor mange tr칝er den m친 lave og l칝rer af.")
        
        boosting_rounds = st.slider("Antal boosting rounds", min_value=1, max_value=100, value=10, step=1)
        st.write("Vi kan ogs친 v칝lge hvor stor en andel af data vi vil bruge. ")
        andel_af_data = st.slider("Andel af data til tr칝ning", min_value=0.001, max_value=1.0, value=1.0, step=0.001)
        
        #Vi omdefinerer vores input og truth data til kun at indeholde en del af dataene.
        input_data_justeret, truth_data_justeret = sklearn.utils.resample(
            input_data, truth_data, 
            n_samples=int(andel_af_data * len(input_data)), 
            random_state=42, 
            replace=False
            )
        st.write("""Vi splitter datas칝ttet i et tr칝ningss칝t og et tests칝t.
Tr칝ningss칝ttet bruges til at tr칝ne modellen, hvor modellen f친r salgspriserne at vide.
Tests칝ttet bruges til at give den tr칝nede model data uden salgspriser, som den s친 skal forudsige, men hvor vi stadig kender svaret.""")
        data_tr칝ning, data_test, sande_pris_tr칝ning, sande_pris_test = \
        sklearn.model_selection.train_test_split(input_data_justeret, truth_data_justeret, test_size=0.25, random_state=42)
    
        # Her bygger vi modellen op med flere tr칝er, tr칝ner p친 data og forudsiger priser
        #Implement button to run below model
        if st.button("K칮r model"):
            gbm_test = lgb.LGBMRegressor(objective='regression', n_estimators=boosting_rounds, verbosity=-1)

            gbm_test.fit(data_tr칝ning, sande_pris_tr칝ning, eval_set=[(data_test, sande_pris_test)], 
                        eval_metric='mse', callbacks=[early_stopping(15)])

            forudsagte_pris = gbm_test.predict(data_test, num_iteration=gbm_test.best_iteration_)
            plotting(sande_pris_test, forudsagte_pris)

            res = sklearn.inspection.permutation_importance(gbm_test, data_test, sande_pris_test, scoring="neg_mean_squared_error")
        
            st.write("Nu vil vi gerne inspicere hvor god vores model er til at forudsige p친 data hvor den ikke kender prisen. Det venstre plot viser residualerne, alts친 sande v칝rdi - forudsagte v칝rdi. Det h칮jre plot er sande v칝rdi vs forudsagte v칝rdi. Her er ogs친 konturer (de sorte linjer), der viser t칝theden af punkterne.")
            st.subheader("Sp칮rgsm친l")
            st.markdown("""
                        - Pr칮v at 칝ndre p친 hvor mange gange gange den m친 booste, ved at 칝ndre boosting_rounds fra 1 til 10, 100 eller 1000. Kan du se en forbedring?
                        - Hvor har modellen sv칝rest ved at forudsige prisen? Er det ved de billigste huse, de dyreste, eller dem i mellem? Hvad kan det v칝re? Hvilke huse tror du der er mest data p친?
                        - Leg rundt med andelen af data du bruger. Hvordan 칝ndres resultatet alt efter hvor meget data den har. Hvor meget data skal du bruge for at have en rimelig model og forudsigelse?""")
            st.subheader("Hvilke variabler er vigtigst?")
            st.write("Vi kan tjekke om vores intuition for hvilke variabler der er vigtigst med 'permutation importance'. Det er et m친l for hvis v칝rdierne i en kolonne bliver byttet rundt randomly, hvor meget p친virker det s친 resultatet. Hvis det er en vigtig variable, vil det p친virke resultatet meget. Her bliver det m친l p친 hvor meget st칮rre mean squared error bliver, n친r den variabel bliver 'scramblet'.")


            imp_mse = res.importances_mean                
            order = np.argsort(imp_mse)[::-1]
            labels = np.asarray(variabler[:-1])[order]
            vals = imp_mse[order]

            fig, ax = plt.subplots(figsize=(8, 6))
            y = np.arange(len(vals))
            ax.barh(y, vals)
            ax.set_yticks(y)
            ax.set_yticklabels(labels)
            ax.set_xlabel("Increase in MSE (permutation)")
            ax.set_ylabel("Feature")
            ax.set_title("Permutation Importance")
            ax.invert_yaxis()
            fig.tight_layout()
            st.pyplot(fig)

            st.write("Er resultatet som du forventede?")

        #NN 
        st.subheader("Neurale Netv칝rk")
        st.write("Neurale Netv칝rk (NN) kommer fra at opbygningen af det, minder om den m친de vores neuroner i hjernen snakker sammen p친. P친 samme m친de som et decision tree er der forskellige lag og vi kan styre hvor mange lag der er, men nu er det ikke kun sandt eller falsk, i stedet fungerer noderne som knapper der kan fintunes. ")
        st.write("Neurale netv칝rk er mere f칮lsomme overfor det data vi giver dem. Den fungerer bedst hvis resultatet er v칝rdier mellem 0 og 1. Derfor bruger vi en funktion til at skalere vores data, kaldet StandardScaler.")
        scaler = sklearn.preprocessing.StandardScaler()
        data_tr칝ning = scaler.fit_transform(data_tr칝ning)
        data_test = scaler.transform(data_test)
        
    
        st.write("I et neuralt netv칝rk kan vi justere p친 hvor mange lag og hvor mange noder hvert lag skal have:")

        #Make six slider, one for each layer. that is six layers in total. sliders decide amount of nodes per layer
        layer_one = st.slider("Antal noder i lag 1", min_value=1, max_value=32, value=32, step=1)
        layer_two = st.slider("Antal noder i lag 2", min_value=1, max_value=32, value=16, step=1)
        layer_three = st.slider("Antal noder i lag 3", min_value=1, max_value=32, value=8, step=1)
        layer_four = st.slider("Antal noder i lag 4", min_value=1, max_value=32, value=4, step=1)
        layer_five = st.slider("Antal noder i lag 5", min_value=1, max_value=32, value=2, step=1)
        layer_six = st.slider("Antal noder i lag 6", min_value=1, max_value=32, value=2, step=1)


        st.write("""Nedenfor tr칝ner vi modellen. Vi kan ogs친 regne ud hvor mange parametre modellen bruger.
Herefter plotter vi for at se hvor godt modellen klarer sig. Denne kan tage op til ~et minut at k칮re.""")
        if st.button("K칮r Neuralt Netv칝rk"):
            # Her definerer og tr칝ner vi modellen
            mlp = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(layer_one, layer_two, layer_three, layer_four, layer_five, layer_six), 
            max_iter=2000, early_stopping=True, random_state=42)
            mlp.fit(data_tr칝ning, sande_pris_tr칝ning) 
            # Her giver vi den tr칝nede model test data som den ikke har set f칮r, og beder om at forudsige prisen
            forudsagte_pris = mlp.predict(data_test)  
            # Beregn antal parametre i modellen
            # Coef er v칝gtene er intercept er bias. Den henter antallet directe fra modellen.
            n_params = sum(coef.size + intercept.size for coef, intercept in zip(mlp.coefs_, mlp.intercepts_))
            st.write(f"Antal parametre i NN: {n_params}")
            plotting(sande_pris_test, forudsagte_pris)
            st.subheader("Sp칮rgsm친l:")
            st.markdown("""- Pr칮v at justere p친 antal neuroner i det neurale netv칝rk - Kan du mindske usikkerheden?
- F친r du det samme antal parameter n친r du regner efter?
- Hvilken algoritme klarer sig bedst? Boosted decision tree eller neutralt netv칝rk?
- Leg rundt med andelen af data du bruger. Hvordan 칝ndres resultatet alt efter hvor meget data den har. Hvor meget data skal du bruge for at have en rimelig model og forudsigelse?""")




    #DIABETES .ipynb
    elif dataset == "Diabetes":
        #HER BEGYNDER VORES .ipynb
        st.subheader("Standard Niveau - Diabetes")
        st.write("Som n칝vnt, har du f친et betroet opgaven at skrive en machine learning algoritme der kan forudsiger om en person har diabetes eller ej. P친 denne hjemmeside beh칮ver vi ikke importere nogen pakker da det er tilrettelagt s친ledes at man skal kunne lege med ML-modellerne uden at skulle bekymre sig om koden bag dem.")

        #Inspicer dataen
        st.subheader("Inspicer dataen")
        st.write("F칮rst vil vi gerne unders칮ge hvilken data vi har med at g칮re.")
        st.dataframe(DS2, height=200, use_container_width=True)
        
        #Tilrettel칝g data
        variabler = DS2.columns
        input_variabler = [v for v in variabler if v != 'Diabetes']
        input_data = DS2[input_variabler].to_numpy()
        truth_data = DS2['Diabetes'].to_numpy()

        st.subheader("Decision Tree")
        st.write("Et decision tree er bygget op af lag og grene. Ved hver gren stiller den et sp칮rgsm친l, og bev칝ger sig ned i det n칝ste lag baseret p친 om sp칮rgsm친let er sandt eller falsk. Og ved at l칝re af en masse data, kan den finde ud af hvilke sp칮rgsm친l der er bedst at stille.")

        st.subheader("Parameter")
        st.write("For et decision tree kan vi justere p친 hvor mange lag der skal v칝re i vores tr칝, alts친 hvor mange lag af sp칮rgsm친l der m친 stilles. Vi kan justere p친 den parameter herunder.")

        #Make a slider to choose depth
        DT_N_lag = st.slider("Antal lag i tr칝et", min_value=1, max_value=10, value=2, step=1)

        st.write("Her bygger og tr칝ner vi modellen og bruger Graphviz til at visualisere det.")

        # Her bliver modellen tr칝net p친 data
        estimator = sklearn.tree.DecisionTreeClassifier(max_depth=DT_N_lag, min_samples_leaf = 20,random_state=42)
        estimator.fit(input_data, truth_data)   # Dette er den "magiske" linje - her optimerer Machine Learning algoritmen sine interne v칝gte til at give bedste svar
        
        # laver visuel graf af tr칝et
        dot = sklearn.tree.export_graphviz(estimator, out_file=None, feature_names=input_variabler, filled=True, max_depth=50, precision=2)         
        dot = dot.replace("squared_error", "error").replace("mse", "error")
        st.graphviz_chart(dot)
        st.write("Max dybde af tr칝et:", estimator.get_depth())


        st.subheader("Sp칮rgsm친l")
        st.markdown("""
- Inspicer tr칝et. Forst친r du/I, hvad de forskellige tal betyder?
  Hvilken kasse falder de fleste personer ned I? hvor er der mindst? 
  
Tallene i value er [raske, diabetikere].
- Hvilke(n) af kasserne bliver kategoriseret som at have diabetes? 
- Hvor stor en del af patienter med diabetes vil den forudsige til at have diabetes?
- Pr칮v at 칝ndre p친 hvor mange lag der er i tr칝et fra 2 til 3.
  Hvilken parameter bliver brugt oftest til at opdele data? Kan du/I ud fra tr칝et sige mere generelt hvilke parametre der betyder mest for om en person har diabetes? Hvilke betyder mindst?
""")
        
        st.subheader("Boosted Decision Tree")
        st.write("Nu hvor vi har set hvordan tr칝et virker, vil vi gerne pr칮ve at forudsige om patienter vi ikke kender diagnosen p친 har diabetes eller ej. Som vi har set, kan det v칝re sv칝rt at minimere vores 'loss function'. En m친de at forbedre p친 er ved at k칮re boosted decision trees, hvilket vil sige at vi k칮rer flere tr칝er, hvor den hver gang l칝rer af fejlene fra det forrige tr칝, og p친 den m친de bliver 'boostet' for hvert tr칝 den laver. Herunder kan vi 칝ndre hvor mange gange den m친 'booste', alts친 hvor mange tr칝er den m친 lave og l칝rer af.")
        
        boosting_rounds = st.slider("Antal boosting rounds", min_value=1, max_value=1000, value=1, step=1)
        st.write("Vi kan ogs친 v칝lge hvor stor en andel af data vi vil bruge. ")
        andel_af_data = st.slider("Andel af data til tr칝ning", min_value=0.001, max_value=1.0, value=1.0, step=0.001)
        
        #Vi omdefinerer vores input og truth data til kun at indeholde en del af dataene.
        input_data_justeret, truth_data_justeret = sklearn.utils.resample(
            input_data, truth_data, 
            n_samples=int(andel_af_data * len(input_data)), 
            random_state=42, 
            replace=False
            )
        st.write("""Vi splitter data i et tr칝ningss칝t og et tests칝t.
Tr칝ningss칝ttet bruges til at tr칝ne modellen, hvor modellen f친r at vide hvilke personer der har diabetes.
Tests칝ttet bruges til at give den tr칝nede model data uden at vide hvilke personer der har diabetes, og bagefter kan vi tjekke om dens forudsigelser var korrekte.""")
        data_tr칝ning, data_test, sand_dybde_tr칝ning, sand_dybde_test = \
    sklearn.model_selection.train_test_split(input_data_justeret, truth_data_justeret, test_size=0.25, random_state=42)
    
        # Her bygger vi modellen op med flere tr칝er, tr칝ner p친 data og forudsiger priser
        #Implement button to run below model
        st.write("Her definerer vi beslutningsgr칝nsen. Som standard bruges 0.5.")
        beslutningsgr칝nse = st.slider("Beslutningsgr칝nse", min_value=0.0, max_value=1.0, value=0.5, step=0.1)
        if st.button("K칮r model"):
            gbm_test = lgb.LGBMClassifier(objective='binary', n_estimators=boosting_rounds, verbosity=-1) 

            # Her tr칝ner vi vores model p친 vores tr칝nings data
            gbm_test.fit(data_tr칝ning, sande_gruppe_tr칝ning, eval_set=[(data_test, sande_gruppe_test)], 
                        callbacks=[early_stopping(15)])

            # Her f친r vi sandsynlighederne for om hver person har diabetes eller ej
            forudsagte_score = gbm_test.predict_proba(data_test, num_iteration=gbm_test.best_iteration_)[:, 1]

            # her f친r vi en liste med 0'er og 1'ere. Hvis en person har en sandsynlighed over beslutningsgr칝nsen, bliver den sat til 1,
            # alts친 forudsagt som diabetiker
            forudsagte_gruppe = gbm_test.predict_proba(data_test, num_iteration=gbm_test.best_iteration_)
            forudsagte_gruppe = (forudsagte_gruppe > beslutningsgr칝nse).astype(int)
            
            Plotting_class(sande_gruppe_test, forudsagte_score, forudsagte_gruppe)

            res = sklearn.inspection.permutation_importance(gbm_test, data_test, sande_gruppe_test, scoring="neg_mean_squared_error")


            st.subheader("Evaluer resultat med AUC og histogram")
            st.write("Nu vil vi gerne inspicere hvor god vores model er til at forudsige om en person har diabetes eller ej. Det venstre plot viser en ROC-kurve dvs. hvor stor en andel af sande g칝t har vi per andel af forkerte g칝t. Jo t칝ttere denne er p친 venstre 칮verste hj칮rne jo bedre. Selve scoren Area Under Curve (AUC) angiver bare hvor t칝t p친 hj칮rnet grafen er. 1 angiver en perfekt score.")
            st.write("Det h칮jre plot viser fordelingen af korrekte og forkerte g칝t farvekodet efter hvad data rent faktisk svarede til. Dvs vi kigger p친 hvad modellen har g칝ttet p친 ud fra hvad vores data rent faktisk svarede til. Der vil altid v칝re nogen der bliver forudsagt forkert, vores opgave er at minimere antallet.")
            st.subheader("Sp칮rgsm친l")
            st.markdown("""
- Pr칮v at 칝ndre p친 hvor mange gange gange den m친 booste, ved at 칝ndre boosting_rounds fra 1 til 10, 100 eller 1000. Kan du se en forbedring?
- Hvor laver modellen flest fejl? Raske der grupperes som diabetikere, eller omvendt?
- Som standard deler den ved 0.5 sandsynlighed. Kunne det v칝re en fordel at dele ved en anden sandsynlighed? (Pr칮v evt. at 칝ndre p친 beslutningsgr칝nsen oppe hvor modellen bliver tr칝net)
- Leg rundt med andelen af data du bruger. Hvordan 칝ndres resultatet alt efter hvor meget data den har. Hvor meget data skal du bruge for at have en rimelig model og forudsigelse?
""")
            st.subheader("Hvilke variabler er vigtigst?")
            st.write("Vi kan tjekke om vores intuition for hvilke variabler der er vigtigst med 'permutation importance'. Det er et m친l for hvis v칝rdierne i en kolonne bliver byttet tilf칝ldigt rundt, hvor meget p친virker det s친 resultatet. Hvis det er en vigtig variabel, vil det p친virke resultatet meget. Her bliver det m친lt p친 hvor meget st칮rre mean squared error (fejlen) bliver, n친r den variabel bliver 'scramblet'.")
            

            imp_mse = res.importances_mean                
            order = np.argsort(imp_mse)[::-1]
            labels = np.asarray(variabler[:-1])[order]
            vals = imp_mse[order]

            fig, ax = plt.subplots(figsize=(8, 6))
            y = np.arange(len(vals))
            ax.barh(y, vals)
            ax.set_yticks(y)
            ax.set_yticklabels(labels)
            ax.set_xlabel("Increase in log_loss (permutation)")
            ax.set_ylabel("Feature")
            ax.set_title("Permutation Importance")
            ax.invert_yaxis()
            fig.tight_layout()
            st.pyplot(fig)

            st.markdown("- Er resultatet som du forventede?")
        #NN 
        st.subheader("Neurale Netv칝rk")
        st.write("Neurale Netv칝rk (NN) kommer fra at opbygningen af det, minder om den m친de vores neuroner i hjernen snakker sammen p친. P친 samme m친de som et decision tree er der forskellige lag og vi kan styre hvor mange lag der er, men nu er det ikke kun sandt eller falsk, i stedet fungerer noderne som knapper der kan fintunes. ")
        st.write("Neurale netv칝rk er mere f칮lsomme overfor det data vi giver dem. Den fungerer bedst hvis resultatet er v칝rdier mellem 0 og 1. Derfor bruger vi en funktion til at skalere eller normalisere vores data, kaldet StandardScaler.")
        scaler = sklearn.preprocessing.StandardScaler()
        data_tr칝ning = scaler.fit_transform(data_tr칝ning)
        data_test = scaler.transform(data_test)
            
        st.write("I et neuralt netv칝rk kan vi justere p친 hvor mange lag og hvor mange noder hvert lag skal have:")

        #Make six slider, one for each layer. that is six layers in total. sliders decide amount of nodes per layer
        layer_one = st.slider("Antal noder i lag 1", min_value=1, max_value=32, value=32, step=1)
        layer_two = st.slider("Antal noder i lag 2", min_value=1, max_value=32, value=16, step=1)
        layer_three = st.slider("Antal noder i lag 3", min_value=1, max_value=32, value=8, step=1)
        layer_four = st.slider("Antal noder i lag 4", min_value=1, max_value=32, value=4, step=1)
        layer_five = st.slider("Antal noder i lag 5", min_value=1, max_value=32, value=2, step=1)
        layer_six = st.slider("Antal noder i lag 6", min_value=1, max_value=32, value=2, step=1)


        st.write("""Nedenfor tr칝ner vi modellen. Vi kan ogs친 regne ud hvor mange parametre modellen bruger.
Herefter plotter vi for at se hvor godt modellen klarer sig.
                 Det kan godt tage op til ~et minut at k칮re denne model.""")
        if st.button("K칮r Neuralt Netv칝rk"):
            # Her definerer og tr칝ner vi modellen
            mlp = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(layer_one, layer_two, layer_three, layer_four, layer_five, layer_six), 
            max_iter=2000, early_stopping=True, random_state=42)
            mlp.fit(data_tr칝ning, sande_gruppe_tr칝ning) 

            # Her giver vi den tr칝nede model test data som den ikke har set f칮r, og beder om at forudsige prisen
            beslutningsgr칝nse = 0.5
            forudsagte_gruppe = mlp.predict_proba(data_test)[:,1]  
            forudsagte_gruppe = (forudsagte_gruppe > beslutningsgr칝nse).astype(int)
            forudsagte_score = mlp.predict_proba(data_test)[:, 1]

            # Beregn antal parametre i modellen
            # Coef er v칝gtene er intercept er bias. Den henter antallet direkte fra modellen.
            n_params = sum(coef.size + intercept.size for coef, intercept in zip(mlp.coefs_, mlp.intercepts_))
            st.write(f"Antal parametre i NN: {n_params}")
            Plotting_class(sande_gruppe_test, forudsagte_score, forudsagte_gruppe)
            st.subheader("Sp칮rgsm친l:")
            st.markdown("""
- Pr칮v at justere p친 antal neuroner i det neurale netv칝rk - Kan du forbedre AUC og antallet at syge klassificeret som raske?
- F친r du det samme antal parameter n친r du regner efter?
- Hvilken algoritme klarer sig bedst? Boosted decision tree eller neutralt netv칝rk?
- Pr칮v at justere p친 dit BDT og NN s친 de rammer samme AUC. Hvilken algoritme er s친 hurtigst?
- Leg rundt med andelen af data du bruger. Hvordan 칝ndres resultatet alt efter hvor meget data den har. Hvor meget data skal du bruge for at have en rimelig model og forudsigelse?
                        """)
            
    elif dataset == "Gletsjer":
        #HER BEGYNDER VORES .ipynb
        st.subheader("Standard Niveau - Gletsjer")
        st.write("Nedenfor skal du hj칝lpe gletsjervidenskabsfakultetet med at udvikle deres ML model til at bestemme dybden af gletsjere. P친 denne hjemmeside beh칮ver vi ikke importere nogen pakker da det er tilrettelagt s친ledes at man skal kunne lege med ML-modellerne uden at skulle bekymre sig om koden bag dem.")

        #Inspicer dataen
        st.subheader("Inspicer dataen")
        st.write("F칮rst vil vi gerne unders칮ge hvilken data vi har med at g칮re.")
        st.dataframe(DS3, height=200, use_container_width=True)
        
        #Tilrettel칝g data
        variabler = DS3.columns
        input_variabler = [v for v in variabler if v != 'gletsjer_dybde']
        input_data = DS3[input_variabler].to_numpy()
        truth_data = DS3['gletsjer_dybde'].to_numpy()

        st.subheader("Decision Tree")
        st.write("Et decision tree er bygget op af lag og grene. Ved hver gren stiller den et sp칮rgsm친l, og bev칝ger sig ned i det n칝ste lag baseret p친 om sp칮rgsm친let er sandt eller falsk. Og ved at l칝re af en masse data, kan den finde ud af hvilke sp칮rgsm친l der er bedst at stille.")

        st.subheader("Parameter")
        st.write("For et decision tree kan vi justere p친 hvor mange lag der skal v칝re i vores tr칝, alts친 hvor mange lag af sp칮rgsm친l der m친 stilles. Vi kan justere p친 den parameter herunder.")

        #Make a slider to choose depth
        DT_N_lag = st.slider("Antal lag i tr칝et", min_value=1, max_value=10, value=2, step=1)

        st.write("Her bygger og tr칝ner vi modellen og bruger Graphviz til at visualisere det.")

        # Her bliver modellen tr칝net p친 data
        estimator = sklearn.tree.DecisionTreeRegressor(max_depth=DT_N_lag, min_samples_leaf = 20,random_state=42)

        estimator.fit(input_data, truth_data)   # Dette er den "magiske" linje - her optimerer Machine Learning algoritmen sine interne v칝gte til at give bedste svar

        # laver visuel graf af tr칝et
        dot = sklearn.tree.export_graphviz(estimator, out_file=None, feature_names=input_variabler, filled=True, max_depth=50, precision=2)         
        dot = dot.replace("squared_error", "error").replace("mse", "error")
        st.graphviz_chart(dot)
        st.write("Max dybde af tr칝et:", estimator.get_depth())
        a = np.unique(estimator.predict(input_data)).size
        st.write("Forskellige priser den kan forudsige:",a )

        st.subheader("Sp칮rgsm친l")
        st.markdown("""- Inspicer tr칝et. Forst친r du/I, hvad de forskellige tal betyder?
  Hvad er g칝ldende for gletsjerne i lag 2 og hvad er algortimens bud p친 deres dybde?
- Pr칮v at 칝ndre p친 hvor mange lag der er i tr칝et fra 2 til 3.
  Hvilken parameter bliver brugt oftest til at opdele data? Tror du/I at den s친 er den vigtigste parameter?
  Kan du/I ud fra tr칝et sige mere generelt hvilke parametre der betyder mest for dybden? Hvilke betyder mindst?""")
        
        st.subheader("Boosted Decision Tree")
        st.write("Nu hvor vi har set hvordan tr칝et virker, vil vi gerne pr칮ve at forudsige v칝rdien p친 gletsjere som vi ikke kender dybden p친. Som vi har set, kan det v칝re sv칝rt at minimere vores 'loss function'. En m친de at forbedre p친 er ved at k칮re boosted decision trees, hvilket vil sige at vi k칮rer flere tr칝er, hvor den hver gang l칝rer af fejlene fra det forrige tr칝, og p친 den m친de bliver 'boostet' for hvert tr칝 den laver. Herunder kan vi 칝ndre hvor mange gange den m친 'booste', alts친 hvor mange tr칝er den m친 lave og l칝re af.")
        
        boosting_rounds = st.slider("Antal boosting rounds", min_value=1, max_value=1000, value=1, step=1)
        st.write("Vi kan ogs친 v칝lge hvor stor en andel af data vi vil bruge. ")
        andel_af_data = st.slider("Andel af data til tr칝ning", min_value=0.001, max_value=1.0, value=1.0, step=0.001)
        
        #Vi omdefinerer vores input og truth data til kun at indeholde en del af dataene.
        input_data_justeret, truth_data_justeret = sklearn.utils.resample(
            input_data, truth_data, 
            n_samples=int(andel_af_data * len(input_data)), 
            random_state=42, 
            replace=False
            )
        st.write("""Vi splitter data i et tr칝ningss칝t og et tests칝t.
Tr칝ningss칝ttet bruges til at tr칝ne modellen, hvor modellen f친r de rigtige dybder at vide at vide.
Tests칝ttet bruges til at give den tr칝nede model data uden dybder, som den s친 skal forudsige, men hvor vi stadig kender svaret. Dette bruges til at evaluere modellens performance.""")
        data_tr칝ning, data_test, sand_dybde_tr칝ning, sand_dybde_test = \
    sklearn.model_selection.train_test_split(input_data_justeret, truth_data_justeret, test_size=0.25, random_state=42)
    
        # Her bygger vi modellen op med flere tr칝er, tr칝ner p친 data og forudsiger priser
        #Implement button to run below model
        if st.button("K칮r model"):
            gbm_test = lgb.LGBMRegressor( objective='regression', n_estimators=boosting_rounds, verbosity=-1)

            gbm_test.fit(data_tr칝ning, sand_dybde_tr칝ning, eval_set=[(data_test, sand_dybde_test)], 
                        eval_metric='mse', callbacks=[early_stopping(15)])

            forudsagt_dybde = gbm_test.predict(data_test, num_iteration=gbm_test.best_iteration_)
            plotting(sand_dybde_test, forudsagt_dybde)

            res = sklearn.inspection.permutation_importance(gbm_test, data_test, sand_dybde_test, scoring="neg_mean_squared_error")

            st.write("Nu vil vi gerne inspicere hvor god vores model er til at forudsige p친 data hvor den ikke kender dybden i forvejen. Det venstre plot viser residualerne, alts친 (sand v칝rdi - forudsagt v칝rdi). Det h칮jre plot er sand v칝rdi vs forudsagt v칝rdi. Her er ogs친 konturer (de sorte linjer), der viser t칝theden af punkterne.")
            st.subheader("Sp칮rgsm친l")
            st.markdown("""
- Pr칮v at 칝ndre p친 hvor mange gange gange den m친 booste, ved at 칝ndre boosting_rounds fra 1 til 2 til 10, 100 eller 1000. Kan du se en forbedring?
- Hvilke gletsjere er der mest data p친?
- Hvad g칝tter modellen p친 hvis ikke den f칮r lov til at booste mange gange? Er der bestemte omr친der hvor modellen har sv칝rere ved at forudsige dybden?
- Leg rundt med andelen af data du bruger. Hvordan 칝ndres resultatet alt efter hvor meget data den har. Hvor meget data skal du bruge for at have en rimelig model og forudsigelse?""")
            st.subheader("Hvilke variable er vigtigst?")
            st.write("Vi kan tjekke om vores intuition for hvilke variable der er vigtigst med 'permutation importance'. Det er et m친l for hvis v칝rdierne i en kolonne bliver byttet rundt randomly, hvor meget p친virker det s친 resultatet. Hvis det er en vigtig variable, vil det p친virke resultatet meget. Her bliver det m친l p친 hvor meget st칮rre mean squared error bliver, n친r den variabel bliver 'scramblet'.")


            imp_mse = res.importances_mean                
            order = np.argsort(imp_mse)[::-1]
            labels = np.asarray(variabler[:-1])[order]
            vals = imp_mse[order]

            fig, ax = plt.subplots(figsize=(8, 6))
            y = np.arange(len(vals))
            ax.barh(y, vals)
            ax.set_yticks(y)
            ax.set_yticklabels(labels)
            ax.set_xlabel("Increase in MSE (permutation)")
            ax.set_ylabel("Feature")
            ax.set_title("Permutation Importance")
            ax.invert_yaxis()
            fig.tight_layout()
            st.pyplot(fig)

            st.markdown("""
- Er resultatet som du forventede? 
- Kan du give en mulig grund til hvorfor netop disse variable har st칮rst betydning?
                        """)

        #NN 
        st.subheader("Neurale Netv칝rk")
        st.write("Neurale Netv칝rk (NN) kommer fra at opbygningen af det, minder om den m친de vores neuroner i hjernen snakker sammen p친. P친 samme m친de som et decision tree er der forskellige lag og vi kan styre hvor mange lag der er, men nu er det ikke kun sandt eller falsk, i stedet fungerer noderne som knapper der kan fintunes.")
        st.write("Neurale netv칝rk er mere f칮lsomme overfor det data vi giver dem. Den fungerer bedst hvis resultatet er v칝rdier mellem 0 og 1. Derfor bruger vi en funktion til at skalere vores data, kaldet StandardScaler.")
        scaler = sklearn.preprocessing.StandardScaler()
        data_tr칝ning = scaler.fit_transform(data_tr칝ning)
        data_test = scaler.transform(data_test)
        
    
        st.write("I et neuralt netv칝rk kan vi justere p친 hvor mange lag og hvor mange noder hvert lag skal have:")

        #Make six slider, one for each layer. that is six layers in total. sliders decide amount of nodes per layer
        layer_one = st.slider("Antal noder i lag 1", min_value=1, max_value=32, value=32, step=1)
        layer_two = st.slider("Antal noder i lag 2", min_value=1, max_value=32, value=16, step=1)
        layer_three = st.slider("Antal noder i lag 3", min_value=1, max_value=32, value=8, step=1)
        layer_four = st.slider("Antal noder i lag 4", min_value=1, max_value=32, value=4, step=1)
        layer_five = st.slider("Antal noder i lag 5", min_value=1, max_value=32, value=2, step=1)
        layer_six = st.slider("Antal noder i lag 6", min_value=1, max_value=32, value=2, step=1)


        st.write("""Nedenfor tr칝ner vi modellen. Vi kan ogs친 regne ud hvor mange parametre modellen bruger.
Herefter plotter vi for at se hvor godt modellen klarer sig.
                 Det kan godt tage op til ~et minut at k칮re denne model.""")
        if st.button("K칮r Neuralt Netv칝rk"):
            # Her definerer og tr칝ner vi modellen
            mlp = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(layer_one, layer_two, layer_three, layer_four, layer_five, layer_six), 
            max_iter=1000, early_stopping=True, random_state=42)
            mlp.fit(data_tr칝ning, sand_dybde_tr칝ning) 
            # Her giver vi den tr칝nede model test data som den ikke har set f칮r, og beder om at forudsige dybden
            forudsagt_dybde = mlp.predict(data_test)  

            # Beregn antal parametre i modellen
            # Coef er v칝gtene er intercept er bias. Den henter antallet directe fra modellen.
            n_params = sum(coef.size + intercept.size for coef, intercept in zip(mlp.coefs_, mlp.intercepts_))
            st.write(f"Antal parametre i NN: {n_params}")
            plotting(sand_dybde_test, forudsagt_dybde)
            st.subheader("Sp칮rgsm친l:")
            st.markdown("""
- Pr칮v at justere p친 antal neuroner i det neurale netv칝rk - Bliver modellen bedre d친rligere/k칮rer den hurtigere langsommere?
- F친r du det samme antal parametre n친r du regner efter?
- Hvilken algoritme klarer sig bedst? Boosted decision tree eller neutralt netv칝rk?
- Leg rundt med andelen af data du bruger. Hvordan 칝ndres resultatet alt efter hvor meget data den har. Hvor meget data skal du bruge for at have en rimelig model og forudsigelse?
                        """)


    #DIABETES .ipynb
    elif dataset == "Partikel":
        #HER BEGYNDER VORES .ipynb
        st.subheader("Standard Niveau - Partikel")
        st.write("Som n칝vnt, er du blevet udn칝vnt personligt til at klassificere elektroner p친 CERN. P친 denne hjemmeside beh칮ver vi ikke importere nogen pakker da det er tilrettelagt s친ledes at man skal kunne lege med ML-modellerne uden at skulle bekymre sig om koden bag dem.")

        #Inspicer dataen
        st.subheader("Inspicer dataen")
        st.write("F칮rst vil vi gerne unders칮ge hvilken data vi har med at g칮re.")
        st.dataframe(DS4, height=200, use_container_width=True)
        
        #Tilrettel칝g data
        variable = DS4.columns
        input_variable = [v for v in variable if v != 'p_Truth_isElectron']
        input_data = DS4[input_variable].to_numpy()
        truth_data = DS4['p_Truth_isElectron'].to_numpy()

        st.subheader("Decision Tree")
        st.write("Et decision tree er bygget op af lag og grene. Ved hver gren stiller den et sp칮rgsm친l, og bev칝ger sig ned i det n칝ste lag baseret p친 om sp칮rgsm친let er sandt eller falsk. Og ved at l칝re af en masse data, kan den finde ud af hvilke sp칮rgsm친l der er bedst at stille.")

        st.subheader("Parameter")
        st.write("For et decision tree kan vi justere p친 hvor mange lag der skal v칝re i vores tr칝, alts친 hvor mange lag af sp칮rgsm친l der m친 stilles. Vi kan justere p친 den parameter herunder.")

        #Make a slider to choose depth
        DT_N_lag = st.slider("Antal lag i tr칝et", min_value=1, max_value=10, value=2, step=1)

        st.write("Her bygger og tr칝ner vi modellen og bruger Graphviz til at visualisere det.")

        # Her bliver modellen tr칝net p친 data
        estimator = sklearn.tree.DecisionTreeClassifier(max_depth=DT_N_lag, min_samples_leaf = 20,random_state=42)
        estimator.fit(input_data, truth_data)   # Dette er den "magiske" linje - her optimerer Machine Learning algoritmen sine interne v칝gte til at give bedste svar

        # laver visuel graf af tr칝et
        dot = sklearn.tree.export_graphviz(estimator, out_file=None, feature_names=input_variable, filled=True, max_depth=50, precision=2)         
        dot = dot.replace("squared_error", "error").replace("mse", "error")
        st.graphviz_chart(dot)
        st.write("Max dybde af tr칝et:", estimator.get_depth())


        st.subheader("Sp칮rgsm친l")
        st.markdown("""
- Inspicer tr칝et. Forst친r du/I, hvad de forskellige tal betyder?
  Hvad sker der fra lag til lag og hvor mange samples er der i hver kasse?
- Pr칮v at 칝ndre p친 hvor mange lag der er i tr칝et fra 2 til 3.
  Hvilke parametre bruges til at opdele data? 
- Hvordan 칝ndres v칝rdien af gini ift. om der kun er elektroner/ikke-elektroner eller begge typer?
                    """)
        
        st.subheader("Boosted Decision Tree")
        st.write("Nu hvor vi har set hvordan tr칝et virker, vil vi gerne pr칮ve at forudsige typen af partikler som vi ikke kender typen af p친 forh친nd. Som vi har set, kan det v칝re sv칝rt at minimere vores 'loss function'. En m친de at forbedre p친 er ved at k칮re boosted decision trees, hvilket vil sige at vi k칮rer flere tr칝er, hvor den hver gang l칝rer af fejlene fra det forrige tr칝, og p친 den m친de bliver 'boostet' for hvert tr칝 den laver. Herunder kan vi 칝ndre hvor mange gange den m친 'booste', alts친 hvor mange tr칝er den m친 lave og l칝rer af.")
        
        boosting_rounds = st.slider("Antal boosting rounds", min_value=1, max_value=1000, value=100, step=1)
        st.write("Vi kan ogs친 v칝lge hvor stor en andel af data vi vil bruge. ")
        andel_af_data = st.slider("Andel af data til tr칝ning", min_value=0.001, max_value=1.0, value=1.0, step=0.001)
        
        #Vi omdefinerer vores input og truth data til kun at indeholde en del af dataene.
        input_data_justeret, truth_data_justeret = sklearn.utils.resample(
            input_data, truth_data, 
            n_samples=int(andel_af_data * len(input_data)), 
            random_state=42, 
            replace=False
            )
        st.write("""Vi splitter data i et tr칝ningss칝t og et tests칝t.
Tr칝ningss칝ttet bruges til at tr칝ne modellen, hvor modellen f친r at vide om data er en elektron eller ej.
Tests칝ttet bruges til at give den tr칝nede model ny data (som den ikke kender svaret til), som den s친 skal forudsige, men hvor vi stadig kender svaret.""")
        data_train, data_test, label_train, label_test = \
    sklearn.model_selection.train_test_split(input_data_justeret, truth_data_justeret, test_size=0.25, random_state=42)
    
        # Her bygger vi modellen op med flere tr칝er, tr칝ner p친 data og forudsiger priser
        #Implement button to run below model
        if st.button("K칮r model"):
            gbm_test = lgb.LGBMClassifier(n_estimators=boosting_rounds,# num_leaves=6,
                              boosting_type='gbdt', objective='binary', 
                              random_state=42)

            gbm_test.fit(data_train, label_train, eval_set=[(data_test, label_test)], 
            callbacks=[early_stopping(15)])

            # Her f친r vi sandsynlighederne for om hver person har diabetes eller ej
            Forudsigelse = gbm_test.predict_proba(data_test, num_iteration=gbm_test.best_iteration_)[:,1]
            
            plotting_partikel(label_test, Forudsigelse)


            st.subheader("Evaluer resultat med AUC og histogram")
            st.write("Nu vil vi gerne inspicere hvor god vores model er til at forusige p친 data hvor den ikke ved om data tilsvarer en elektron eller ej. Det venstre plot viser en ROC-kurve dvs. hvor stor en andel af sande g칝t har vi per andel af forkerte g칝t. Jo t칝ttere denne er p친 venstre 칮verste hj칮rne jo bedre. Dvs. n친r raten af forkerte g칝t er 0.1 er raten af korrekte g칝t allrede omkring 0.9.")
            st.write("Selve scoren Area Under Curve (AUC) angiver bare hvor t칝t p친 hj칮rnet grafen er. 1 angiver en perfekt score.")
            st.write("Det h칮jre plot viser fordelingen af korrekte og forkerte g칝t farvekodet efter hvad data rent faktisk svarede til. Dvs vi kigger p친 hvad modellen har g칝ttet p친 ud fra hvad vores data rent faktisk svarede til.")
            st.subheader("Sp칮rgsm친l")
            st.markdown("""
- 칁ndr p친 antallet af boosting_rounds og se hvad der sker med modellene og resultatet. Kan du se forskel i performance for f.eks. 1, 10, 100, 1000 boosting_rounds?
- Hvad sker der med fordelingen af data i h칮jre plot n친r du 칝ndrer p친 boosting_rounds? Kan du stadig godt klassificere elektroner ved boosting_rounds=1 eller boosting_rounds=10? (Bem칝rk den stiplede linje er defineret ved 0.5 og modellen har ikke indflydelse p친 den.)
- Leg rundt med andelen af data du bruger. Hvordan 칝ndres resultatet alt efter hvor meget data den har. Hvor meget data skal du bruge for at have en rimelig model og forudsigelse?
                        """)
            st.subheader("Hvilke variabler er vigtigst?")
            st.write("Vi kan tjekke om vores hvilke variabler der er vigtigst for modellen til at lave en forudsigelse med 'permutation importance'. Det er et m친l for hvis v칝rdierne i en kolonne bliver byttet rundt randomly, hvor meget p친virker det s친 resultatet. Hvis det er en vigtig variabel, vil det p친virke resultatet meget. Her bliver det m친l p친 hvor meget st칮rre mean squared error bliver, n친r den variabel bliver 'scramblet'.")
            

            perm_importance = sklearn.inspection.permutation_importance(gbm_test, data_test, label_test,scoring='neg_log_loss', random_state=42)
            order = perm_importance.importances_mean.argsort()[::1]
            labels = np.asarray(variable[:-1])[order]
            vals = perm_importance.importances_mean[order]
            
            fig, ax = plt.subplots(figsize=(8, 6))
            y = np.arange(len(vals))
            ax.barh(y, vals)
            ax.set_yticks(y)
            ax.set_yticklabels(labels)
            ax.set_xlabel("Increase in log_loss (permutation)")
            ax.set_ylabel("Feature")
            ax.set_title("Permutation Importance")
            ax.invert_yaxis()
            fig.tight_layout()
            st.pyplot(fig)

        #NN 
        st.subheader("Neurale Netv칝rk")
        st.write("Neurale Netv칝rk (NN) kommer fra at opbygningen af det, minder om den m친de vores neuroner i hjernen snakker sammen p친. P친 samme m친de som et decision tree er der forskellige lag og vi kan styre hvor mange lag der er, men nu er det ikke kun sandt eller falsk, i stedet fungerer noderne som knapper der kan fintunes. ")
        st.write("Neurale netv칝rk er mere f칮lsomme overfor det data vi giver dem. Den fungerer bedst hvis data er v칝rdier mellem 0 og 1. Derfor bruger vi en funktion til at skalere vores data, kaldet StandardScaler.")
        scaler = sklearn.preprocessing.StandardScaler()
        data_train_transformed = scaler.fit_transform(data_train)
        data_test_transformed = scaler.transform(data_test)    
        
        st.write("I et neuralt netv칝rk kan vi justere p친 hvor mange lag og hvor mange noder hvert lag skal have:")

        #Make six slider, one for each layer. that is six layers in total. sliders decide amount of nodes per layer
        layer_one = st.slider("Antal noder i lag 1", min_value=1, max_value=32, value=32, step=1)
        layer_two = st.slider("Antal noder i lag 2", min_value=1, max_value=32, value=16, step=1)
        layer_three = st.slider("Antal noder i lag 3", min_value=1, max_value=32, value=8, step=1)
        layer_four = st.slider("Antal noder i lag 4", min_value=1, max_value=32, value=4, step=1)
        layer_five = st.slider("Antal noder i lag 5", min_value=1, max_value=32, value=2, step=1)
        layer_six = st.slider("Antal noder i lag 6", min_value=1, max_value=32, value=2, step=1)


        st.write("""Nedenfor tr칝ner vi modellen. Vi kan ogs친 regne ud hvor mange parametre modellen bruger.
Herefter plotter vi for at se hvor godt modellen klarer sig.
                 Det kan godt tage op til ~et minut at k칮re denne model.""")
        if st.button("K칮r Neuralt Netv칝rk"):
            # Her definerer og tr칝ner vi modellen
            mlp = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(layer_one, layer_two, layer_three, layer_four, layer_five, layer_six), 
            max_iter=2000, early_stopping=True, random_state=42)
            mlp.fit(data_train_transformed, label_train) 

            # Her giver vi den tr칝nede model test data som den ikke har set f칮r, og beder om at forudsige prisen
            Forudsigelse = mlp.predict_proba(data_test_transformed)[:,1]

            # Beregn antal parametre i modellen
            # Coef er v칝gtene er intercept er bias. Den henter antallet direkte fra modellen.
            n_params = sum(coef.size + intercept.size for coef, intercept in zip(mlp.coefs_, mlp.intercepts_))
            st.write(f"Antal parametre i NN: {n_params}")
            plotting_partikel(label_test, Forudsigelse)
            st.subheader("Sp칮rgsm친l:")
            st.markdown("""
- Sammenlign modellen med boosted decision tree ovenover. Hvilken algoritme klarer sig bedst?
- 칁ndr antallet af neuroner per lag/antallet af lag og se hvordan performance 칝ndrer sig.
- F친r du det samme antal parameter n친r du regner efter?
- Leg rundt med andelen af data du bruger. Hvordan 칝ndres resultatet alt efter hvor meget data den har. Hvor meget data skal du bruge for at have en rimelig model og forudsigelse?""")        
    # elif dataset == "Upload dit eget datas칝t":
    #     st.subheader("Standard Niveau - Upload dit eget datas칝t")
    #     st.write("Indhold for Standard Niveau og Upload dit eget datas칝t.")
    #     # Add standard-level content for uploaded dataset here

if __name__ == "__main__":
    main()